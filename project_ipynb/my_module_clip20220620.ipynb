{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/key2317/anaconda3/envs/CLIP4Clip/lib/python3.8/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "\n",
    "import logging\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "\n",
    "from modules.until_module import PreTrainedModel, AllGather, CrossEn\n",
    "from modules.module_cross import CrossModel, CrossConfig, Transformer as TransformerClip\n",
    "\n",
    "from modules.module_clip import CLIP, convert_weights\n",
    "from torch.nn.utils.rnn import pad_packed_sequence, pack_padded_sequence\n",
    "\n",
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import unicode_literals\n",
    "from __future__ import print_function\n",
    "\n",
    "import os\n",
    "from torch.utils.data import Dataset\n",
    "import numpy as np\n",
    "import pickle\n",
    "from dataloaders.rawvideo_util import RawVideoExtractor\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "from dataloaders.dataloader_msrvtt_retrieval import MSRVTT_DataLoader\n",
    "from dataloaders.dataloader_msrvtt_retrieval import MSRVTT_TrainDataLoader\n",
    "from dataloaders.dataloader_msvd_retrieval import MSVD_DataLoader\n",
    "from dataloaders.dataloader_lsmdc_retrieval import LSMDC_DataLoader\n",
    "from dataloaders.dataloader_activitynet_retrieval import ActivityNet_DataLoader\n",
    "from dataloaders.dataloader_didemo_retrieval import DiDeMo_DataLoader\n",
    "from modules.tokenization_clip import SimpleTokenizer as ClipTokenizer\n",
    "from dataloaders.data_dataloaders import DATALOADER_DICT\n",
    "\n",
    "import collections \n",
    "from collections import OrderedDict\n",
    "from torch import nn\n",
    "import import_ipynb \n",
    "\n",
    "from collections import OrderedDict\n",
    "from typing import Tuple, Union\n",
    "\n",
    "import hashlib\n",
    "import os\n",
    "import urllib\n",
    "import warnings\n",
    "from tqdm import tqdm\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch import nn\n",
    "\n",
    "#### 20220610 추가 \n",
    "from modules.file_utils import PYTORCH_PRETRAINED_BERT_CACHE\n",
    "from metrics import compute_metrics, tensor_text_to_video_metrics, tensor_video_to_text_sim\n",
    "from modules.modeling import CLIP4Clip\n",
    "from modules.optimization import BertAdam\n",
    "import time\n",
    "\n",
    "from util import parallel_apply, get_logger\n",
    "from dataloaders.data_dataloaders import DATALOADER_DICT\n",
    "\n",
    "\n",
    "#### 20220615 추가 #### \n",
    "import torch.multiprocessing as mp\n",
    "import torch.distributed as dist\n",
    "from torch.nn.parallel import DistributedDataParallel as DDP\n",
    "import _random\n",
    "import sys\n",
    "\n",
    "\n",
    "\n",
    "#### 20220615 추가 #### \n",
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "\n",
    "import os\n",
    "import copy\n",
    "import json\n",
    "import math\n",
    "import logging\n",
    "import tarfile\n",
    "import tempfile\n",
    "import shutil\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "from collections import OrderedDict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "_MODELS = {\n",
    "    \"RN50\": \"https://openaipublic.azureedge.net/clip/models/afeb0e10f9e5a86da6080e35cf09123aca3b358a0c3e3b6c78a7b63bc04b6762/RN50.pt\",\n",
    "    \"RN101\": \"https://openaipublic.azureedge.net/clip/models/8fa8567bab74a42d41c5915025a8e4538c3bdbe8804a470a72f30b0d94fab599/RN101.pt\",\n",
    "    \"RN50x4\": \"https://openaipublic.azureedge.net/clip/models/7e526bd135e493cef0776de27d5f42653e6b4c8bf9e0f653bb11773263205fdd/RN50x4.pt\",\n",
    "    \"RN50x16\": \"https://openaipublic.azureedge.net/clip/models/52378b407f34354e150460fe41077663dd5b39c54cd0bfd2b27167a4a06ec9aa/RN50x16.pt\",\n",
    "    \"ViT-B/32\": \"https://openaipublic.azureedge.net/clip/models/40d365715913c9da98579312b702a82c18be219cc2a73407c4526f58eba950af/ViT-B-32.pt\",\n",
    "    \"ViT-B/16\": \"https://openaipublic.azureedge.net/clip/models/5806e77cd80f8b59890b7e101eabd078d9fb84e6937f9e85e4ecb61988df416f/ViT-B-16.pt\",\n",
    "}\n",
    "_PT_NAME = {\n",
    "    \"RN50\": \"RN50.pt\",\n",
    "    \"RN101\": \"RN101.pt\",\n",
    "    \"RN50x4\": \"RN50x4.pt\",\n",
    "    \"RN50x16\": \"RN50x16.pt\",\n",
    "    \"ViT-B/32\": \"ViT-B-32.pt\",\n",
    "    \"ViT-B/16\": \"ViT-B-16.pt\",\n",
    "}\n",
    "def _download(url: str, root: str = os.path.expanduser(\"~/.cache/clip\")):\n",
    "    os.makedirs(root, exist_ok=True)\n",
    "    filename = os.path.basename(url)\n",
    "\n",
    "    expected_sha256 = url.split(\"/\")[-2]\n",
    "    download_target = os.path.join(root, filename)\n",
    "\n",
    "    if os.path.exists(download_target) and not os.path.isfile(download_target):\n",
    "        raise RuntimeError(f\"{download_target} exists and is not a regular file\")\n",
    "\n",
    "    if os.path.isfile(download_target):\n",
    "        if hashlib.sha256(open(download_target, \"rb\").read()).hexdigest() == expected_sha256:\n",
    "            return download_target\n",
    "        else:\n",
    "            warnings.warn(f\"{download_target} exists, but the SHA256 checksum does not match; re-downloading the file\")\n",
    "\n",
    "    with urllib.request.urlopen(url) as source, open(download_target, \"wb\") as output:\n",
    "        with tqdm(total=int(source.info().get(\"Content-Length\")), ncols=80, unit='iB', unit_scale=True) as loop:\n",
    "            while True:\n",
    "                buffer = source.read(8192)\n",
    "                if not buffer:\n",
    "                    break\n",
    "\n",
    "                output.write(buffer)\n",
    "                loop.update(len(buffer))\n",
    "\n",
    "    if hashlib.sha256(open(download_target, \"rb\").read()).hexdigest() != expected_sha256:\n",
    "        raise RuntimeError(f\"Model has been downloaded but the SHA256 checksum does not not match\")\n",
    "\n",
    "    return download_target\n",
    "\n",
    "def available_models():\n",
    "    \"\"\"Returns the names of available CLIP models\"\"\"\n",
    "    return list(_MODELS.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'data_path': '/home/key2317/CLIP4Clip/msvd_data', 'features_path': '/home/key2317/CLIP4Clip/msvd_data/MSVD_Videos', 'max_words': 30, 'feature_framerate': 1, 'max_frames': 16, 'image_resolution': 224, 'frame_order': 0, 'slice_framepos': 0, 'train_frame_order': 0, 'batch_size': 256, 'n_gpu': 4, 'num_thread_reader': 1, 'datatype': 'msvd', 'eval_frame_order': 0, 'batch_size_val': 3500, 'local_rank': 0, 'init_model': '', 'cache_dir': '/home/key2317/CLIP4Clip/CLIP4Clip_original', 'cross_model': 'cross-base', 'gradient_accumulation_steps': 1, 'epochs': 1, 'seed': 42, 'world_size': 1}\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import easydict \n",
    "DATA_PATH = \"/home/key2317/CLIP4Clip/msvd_data\"\n",
    "FEATURES_PATH = \"/home/key2317/CLIP4Clip/msvd_data/MSVD_Videos\"\n",
    "\n",
    "descriptions = ''\n",
    "local_rank = 1\n",
    "cross_model_name = 'cross-base'\n",
    "n_gpu=1\n",
    "cache_dir=\"\"\n",
    "pretrained_clip_name = \"ViT-B/32\"\n",
    "\n",
    "\n",
    "# 회의 get_config는 module_clip 내에 선언되어 있는 CLIP 클래스에 정의되어 있음.\n",
    "clip_state_dict = CLIP.get_config(pretrained_clip_name=pretrained_clip_name)\n",
    "CONFIG_NAME = 'cross_config.json'\n",
    "#print(clip_state_dict.keys())\n",
    "action = 'store_true'\n",
    "import easydict \n",
    "DATA_PATH = \"/home/key2317/CLIP4Clip/msvd_data\"\n",
    "FEATURES_PATH = \"/home/key2317/CLIP4Clip/msvd_data/MSVD_Videos\"\n",
    "args = easydict.EasyDict({\n",
    "    \"data_path\":DATA_PATH,\n",
    "    \"features_path\":FEATURES_PATH,\n",
    "    \"max_words\":30,\n",
    "    \"feature_framerate\":1,\n",
    "    \"max_frames\":16, #### \n",
    "    \"image_resolution\":224,\n",
    "    \"frame_order\":0,\n",
    "    \"slice_framepos\":0,\n",
    "    \"train_frame_order\":0, #default 0, choice = [0,1,2]\n",
    "    \"batch_size\":256,\n",
    "    \"n_gpu\":4, #default :1, torch.cuda.device_count() \n",
    "    \"num_thread_reader\":1,\n",
    "    \"datatype\":\"msvd\",\n",
    "    \"eval_frame_order\":0, #choices = [0, 1, 2]\n",
    "    \"batch_size_val\":3500,\n",
    "    \"local_rank\":0,\n",
    "\n",
    "    ## 20220610 추가\n",
    "    \"init_model\":\"\",\n",
    "    \"cache_dir\":\"/home/key2317/CLIP4Clip/CLIP4Clip_original\",\n",
    "    \"cross_model\":\"cross-base\", #default\n",
    "    \"gradient_accumulation_steps\":1,\n",
    "    \"epochs\":1,\n",
    "\n",
    "    ## 20220615 추가 \n",
    "    \"seed\":42,\n",
    "\n",
    "    ## 20220617 추가 \n",
    "    \"world_size\":1,\n",
    "\n",
    "})\n",
    "\n",
    "type_vocab_size = 2 \n",
    "task_config = args\n",
    "cross_config, _ = CrossConfig.get_config(cross_model_name, cache_dir, type_vocab_size, state_dict=None, task_config=task_config)\n",
    "#print(args.__dict__)\n",
    "tokenizer = ClipTokenizer()\n",
    "#train_dataloader, train_length, train_sampler = DATALOADER_DICT[args.datatype][\"train\"](args, tokenizer)\n",
    "print(args.__dict__)\n",
    "tokenizer = ClipTokenizer()\n",
    "#train_dataloader, train_length, train_sampler = DATALOADER_DICT[args.datatype][\"train\"](args, tokenizer)\n",
    "cut_top_layer = 0\n",
    "\n",
    "logger = logging.getLogger(__name__)\n",
    "allgather = AllGather.apply\n",
    "def print_shape(target):\n",
    "    print(\"current shape {}\".format(target.shape))\n",
    "\n",
    "@property\n",
    "def dtype(self):\n",
    "    return self.visual.conv1.weight.dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. BottleNeck"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Bottleneck(nn.Module):\n",
    "    expansion = 4\n",
    "\n",
    "    def __init__(self, inplanes, planes, stride=1):\n",
    "        super().__init__()\n",
    "\n",
    "        # all conv layers have stride 1. an avgpool is performed after the second convolution when stride > 1\n",
    "        self.conv1 = nn.Conv2d(inplanes, planes, 1, bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(planes)\n",
    "\n",
    "        self.conv2 = nn.Conv2d(planes, planes, 3, padding=1, bias=False)\n",
    "        self.bn2 = nn.BatchNorm2d(planes)\n",
    "\n",
    "        self.avgpool = nn.AvgPool2d(stride) if stride > 1 else nn.Identity()\n",
    "\n",
    "        self.conv3 = nn.Conv2d(planes, planes * self.expansion, 1, bias=False)\n",
    "        self.bn3 = nn.BatchNorm2d(planes * self.expansion)\n",
    "\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        self.downsample = None\n",
    "        self.stride = stride\n",
    "\n",
    "        if stride > 1 or inplanes != planes * Bottleneck.expansion:\n",
    "            # downsampling layer is prepended with an avgpool, and the subsequent convolution has stride 1\n",
    "            self.downsample = nn.Sequential(OrderedDict([\n",
    "                (\"-1\", nn.AvgPool2d(stride)),\n",
    "                (\"0\", nn.Conv2d(inplanes, planes * self.expansion, 1, stride=1, bias=False)),\n",
    "                (\"1\", nn.BatchNorm2d(planes * self.expansion))\n",
    "            ]))\n",
    "\n",
    "    def forward(self, x: torch.Tensor):\n",
    "        identity = x\n",
    "\n",
    "        out = self.relu(self.bn1(self.conv1(x)))\n",
    "        out = self.relu(self.bn2(self.conv2(out)))\n",
    "        out = self.avgpool(out)\n",
    "        out = self.bn3(self.conv3(out))\n",
    "\n",
    "        if self.downsample is not None:\n",
    "            identity = self.downsample(x)\n",
    "\n",
    "        out += identity\n",
    "        out = self.relu(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. ModifiedResNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ModifiedResNet(nn.Module):\n",
    "    \"\"\"\n",
    "    A ResNet class that is similar to torchvision's but contains the following changes:\n",
    "    - There are now 3 \"stem\" convolutions as opposed to 1, with an average pool instead of a max pool.\n",
    "    - Performs anti-aliasing strided convolutions, where an avgpool is prepended to convolutions with stride > 1\n",
    "    - The final pooling layer is a QKV attention instead of an average pool\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, layers, output_dim, heads, input_resolution=224, width=64):\n",
    "        super().__init__()\n",
    "        self.output_dim = output_dim\n",
    "        self.input_resolution = input_resolution\n",
    "\n",
    "        # the 3-layer stem\n",
    "        self.conv1 = nn.Conv2d(3, width // 2, kernel_size=3, stride=2, padding=1, bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(width // 2)\n",
    "        self.conv2 = nn.Conv2d(width // 2, width // 2, kernel_size=3, padding=1, bias=False)\n",
    "        self.bn2 = nn.BatchNorm2d(width // 2)\n",
    "        self.conv3 = nn.Conv2d(width // 2, width, kernel_size=3, padding=1, bias=False)\n",
    "        self.bn3 = nn.BatchNorm2d(width)\n",
    "        self.avgpool = nn.AvgPool2d(2)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "\n",
    "        # residual layers\n",
    "        self._inplanes = width  # this is a *mutable* variable used during construction\n",
    "        self.layer1 = self._make_layer(width, layers[0])\n",
    "        self.layer2 = self._make_layer(width * 2, layers[1], stride=2)\n",
    "        self.layer3 = self._make_layer(width * 4, layers[2], stride=2)\n",
    "        self.layer4 = self._make_layer(width * 8, layers[3], stride=2)\n",
    "\n",
    "        embed_dim = width * 32  # the ResNet feature dimension\n",
    "        self.attnpool = AttentionPool2d(input_resolution // 32, embed_dim, heads, output_dim)\n",
    "\n",
    "    def _make_layer(self, planes, blocks, stride=1):\n",
    "        layers = [Bottleneck(self._inplanes, planes, stride)]\n",
    "\n",
    "        self._inplanes = planes * Bottleneck.expansion\n",
    "        for _ in range(1, blocks):\n",
    "            layers.append(Bottleneck(self._inplanes, planes))\n",
    "\n",
    "        return nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        def stem(x):\n",
    "            for conv, bn in [(self.conv1, self.bn1), (self.conv2, self.bn2), (self.conv3, self.bn3)]:\n",
    "                x = self.relu(bn(conv(x)))\n",
    "            x = self.avgpool(x)\n",
    "            return x\n",
    "\n",
    "        x = x.type(self.conv1.weight.dtype)\n",
    "        x = stem(x)\n",
    "        x = self.layer1(x)\n",
    "        x = self.layer2(x)\n",
    "        x = self.layer3(x)\n",
    "        x = self.layer4(x)\n",
    "        x = self.attnpool(x)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. CLIP4ClipPreTrainedModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CLIP4ClipPreTrainedModel(PreTrainedModel, nn.Module):\n",
    "    \"\"\" An abstract class to handle weights initialization and\n",
    "        a simple interface for dowloading and loading pretrained models.\n",
    "    \"\"\"\n",
    "    def __init__(self, cross_config, *inputs, **kwargs):\n",
    "        super(CLIP4ClipPreTrainedModel, self).__init__(cross_config)\n",
    "        self.cross_config = cross_config\n",
    "        self.clip = None\n",
    "        self.cross = None\n",
    "\n",
    "    @classmethod\n",
    "    def from_pretrained(cls, cross_model_name, state_dict=None, cache_dir=None, type_vocab_size=2, *inputs, **kwargs):\n",
    "\n",
    "        task_config = None\n",
    "        if \"task_config\" in kwargs.keys():\n",
    "            task_config = kwargs[\"task_config\"]\n",
    "            ###### 20220524 local rank\n",
    "            if not hasattr(task_config, \"local_rank\"):\n",
    "                #args.__dict__[\"max_frames\"]출력하면 100 나옴을 확인\n",
    "                task_config.__dict__[\"local_rank\"] = 0\n",
    "            elif task_config.local_rank == -1:\n",
    "                task_config.local_rank = 0\n",
    "\n",
    "        if state_dict is None: state_dict = {}\n",
    "        pretrained_clip_name = \"ViT-B/32\"\n",
    "        if hasattr(task_config, 'pretrained_clip_name'):\n",
    "            pretrained_clip_name = task_config.pretrained_clip_name\n",
    "        clip_state_dict = CLIP.get_config(pretrained_clip_name=pretrained_clip_name)\n",
    "        for key, val in clip_state_dict.items():\n",
    "            new_key = \"clip.\" + key\n",
    "            if new_key not in state_dict:\n",
    "                state_dict[new_key] = val.clone()\n",
    "\n",
    "        cross_config, _ = CrossConfig.get_config(cross_model_name, cache_dir, type_vocab_size, state_dict=None, task_config=task_config)\n",
    "\n",
    "        model = cls(cross_config, clip_state_dict, *inputs, **kwargs)\n",
    "\n",
    "        ## ===> Initialization trick [HARD CODE]\n",
    "        if model.linear_patch == \"3d\":\n",
    "            contain_conv2 = False\n",
    "            for key in state_dict.keys():\n",
    "                if key.find(\"visual.conv2.weight\") > -1:\n",
    "                    contain_conv2 = True\n",
    "                    break\n",
    "            if contain_conv2 is False and hasattr(model.clip.visual, \"conv2\"):\n",
    "                cp_weight = state_dict[\"clip.visual.conv1.weight\"].clone()\n",
    "                kernel_size = model.clip.visual.conv2.weight.size(2)\n",
    "                conv2_size = model.clip.visual.conv2.weight.size()\n",
    "                conv2_size = list(conv2_size)\n",
    "\n",
    "                left_conv2_size = conv2_size.copy()\n",
    "                right_conv2_size = conv2_size.copy()\n",
    "                left_conv2_size[2] = (kernel_size - 1) // 2\n",
    "                right_conv2_size[2] = kernel_size - 1 - left_conv2_size[2]\n",
    "\n",
    "                left_zeros, right_zeros = None, None\n",
    "                if left_conv2_size[2] > 0:\n",
    "                    left_zeros = torch.zeros(*tuple(left_conv2_size), dtype=cp_weight.dtype, device=cp_weight.device)\n",
    "                if right_conv2_size[2] > 0:\n",
    "                    right_zeros = torch.zeros(*tuple(right_conv2_size), dtype=cp_weight.dtype, device=cp_weight.device)\n",
    "\n",
    "                cat_list = []\n",
    "                if left_zeros != None: cat_list.append(left_zeros)\n",
    "                cat_list.append(cp_weight.unsqueeze(2))\n",
    "                if right_zeros != None: cat_list.append(right_zeros)\n",
    "                cp_weight = torch.cat(cat_list, dim=2)\n",
    "\n",
    "                state_dict[\"clip.visual.conv2.weight\"] = cp_weight\n",
    "\n",
    "        if model.sim_header == 'tightTransf':\n",
    "            contain_cross = False\n",
    "            for key in state_dict.keys():\n",
    "                if key.find(\"cross.transformer\") > -1:\n",
    "                    contain_cross = True\n",
    "                    break\n",
    "            if contain_cross is False:\n",
    "                for key, val in clip_state_dict.items():\n",
    "                    if key == \"positional_embedding\":\n",
    "                        state_dict[\"cross.embeddings.position_embeddings.weight\"] = val.clone()\n",
    "                        continue\n",
    "                    if key.find(\"transformer.resblocks\") == 0:\n",
    "                        num_layer = int(key.split(\".\")[2])\n",
    "\n",
    "                        # cut from beginning\n",
    "                        if num_layer < task_config.cross_num_hidden_layers:\n",
    "                            state_dict[\"cross.\"+key] = val.clone()\n",
    "                            continue\n",
    "\n",
    "        if model.sim_header == \"seqLSTM\" or model.sim_header == \"seqTransf\":\n",
    "            contain_frame_position = False\n",
    "            for key in state_dict.keys():\n",
    "                if key.find(\"frame_position_embeddings\") > -1:\n",
    "                    contain_frame_position = True\n",
    "                    break\n",
    "            if contain_frame_position is False:\n",
    "                for key, val in clip_state_dict.items():\n",
    "                    if key == \"positional_embedding\":\n",
    "                        state_dict[\"frame_position_embeddings.weight\"] = val.clone()\n",
    "                        continue\n",
    "                    if model.sim_header == \"seqTransf\" and key.find(\"transformer.resblocks\") == 0:\n",
    "                        num_layer = int(key.split(\".\")[2])\n",
    "                        # cut from beginning\n",
    "                        if num_layer < task_config.cross_num_hidden_layers:\n",
    "                            state_dict[key.replace(\"transformer.\", \"transformerClip.\")] = val.clone()\n",
    "                            continue\n",
    "        ## <=== End of initialization trick\n",
    "\n",
    "        if state_dict is not None:\n",
    "            model = cls.init_preweight(model, state_dict, task_config=task_config)\n",
    "\n",
    "        return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. Task_config와 Target_config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_log(task_config, info):\n",
    "    if task_config is None or task_config.local_rank == 0:\n",
    "        logger.warning(info)\n",
    "\n",
    "def update_attr(target_name, target_config, target_attr_name, source_config, source_attr_name, default_value=None):\n",
    "    if hasattr(source_config, source_attr_name):\n",
    "        if default_value is None or getattr(source_config, source_attr_name) != default_value:\n",
    "            setattr(target_config, target_attr_name, getattr(source_config, source_attr_name))\n",
    "            show_log(source_config, \"Set {}.{}: {}.\".format(target_name,\n",
    "                                                            target_attr_name, getattr(target_config, target_attr_name)))\n",
    "    return target_config\n",
    "\n",
    "def check_attr(target_name, task_config):\n",
    "    return hasattr(task_config, target_name) and task_config.__dict__[target_name]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5. DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dataloader_msvd_train(args,tokenizer):\n",
    "    msvd_dataset=MSVD_DataLoader(\n",
    "        subset = \"train\",\n",
    "        data_path = args.data_path,\n",
    "        features_path = args.features_path,\n",
    "        max_words = args.max_words,\n",
    "        feature_framerate=args.feature_framerate,\n",
    "        tokenizer = tokenizer,\n",
    "        max_frames=args.max_frames,\n",
    "        frame_order = args.train_frame_order, \n",
    "        slice_framepos = args.slice_framepos\n",
    "    )\n",
    "\n",
    "    #t)\n",
    "    #train_sampler = rain_sampler = torch.utils.data.distributed.DistributedSampler(msvd_dataset)\n",
    "    train_sampler = 0\n",
    "    dataloader = DataLoader(\n",
    "        msvd_dataset,\n",
    "        batch_size = args.batch_size // args.n_gpu, \n",
    "        num_workers = args.num_thread_reader,\n",
    "        pin_memory=False, \n",
    "        shuffle = (train_sampler is None), \n",
    "        sampler = train_sampler, \n",
    "        drop_last=True,\n",
    "\n",
    "        \n",
    "    )\n",
    "\n",
    "    return msvd_dataset, dataloader, len(msvd_dataset),train_sampler\n",
    "\n",
    "def dataloader_msvd_test(args, tokenizer, subset=\"test\"):\n",
    "    msvd_testset = MSVD_DataLoader(\n",
    "        subset=subset,\n",
    "        data_path=args.data_path,\n",
    "        features_path=args.features_path,\n",
    "        max_words=args.max_words,\n",
    "        feature_framerate=args.feature_framerate,\n",
    "        tokenizer=tokenizer,\n",
    "        max_frames=args.max_frames,\n",
    "        frame_order=args.eval_frame_order,\n",
    "        slice_framepos=args.slice_framepos,\n",
    "    )\n",
    "    dataloader_msvd = DataLoader(\n",
    "        msvd_testset,\n",
    "        batch_size=args.batch_size_val,\n",
    "        num_workers=args.num_thread_reader,\n",
    "        shuffle=False,\n",
    "        drop_last=False,\n",
    "    )\n",
    "    return msvd_testset, dataloader_msvd, len(msvd_testset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Video number: 1200\n",
      "Total Paire: 48774\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/key2317/anaconda3/envs/CLIP4Clip/lib/python3.8/site-packages/torchvision/transforms/transforms.py:257: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For test, sentence number: 27763\n",
      "For test, video number: 670\n",
      "Video number: 670\n",
      "Total Paire: 27763\n"
     ]
    }
   ],
   "source": [
    "msvd_dataset,train_dataloader,train_length,train_sampler = dataloader_msvd_train(args,tokenizer)\n",
    "msvd_testset, test_dataloader, test_length = dataloader_msvd_test(args,tokenizer,subset=\"test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "6. AttentionPool2D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AttentionPool2d(nn.Module):\n",
    "\n",
    "    def __init__(self, spacial_dim: int, embed_dim : int, num_heads : int, output_dim : int = None):\n",
    "        super(AttentionPool2d,self).__init__()\n",
    "        #spacial_dim의 제곱 + 1 , embed_dim \n",
    "        self.positional_embedding = nn.Parameter(torch.randn(spacial_dim**2 +1 ,embed_dim) / embed_dim ** 0.5)\n",
    "        print(self.positional_embedding.shape)\n",
    "        # Key, Query, Value\n",
    "        self.k_proj = nn.Linear(embed_dim,embed_dim)\n",
    "        self.q_proj = nn.Linear(embed_dim,embed_dim)\n",
    "        self.v_proj = nn.Linear(embed_dim,embed_dim)\n",
    "        \n",
    "        self.c_proj = nn.Linear(embed_dim,output_dim or embed_dim)\n",
    "        self.num_heads = num_heads \n",
    "\n",
    "    def forward(self,x):\n",
    "        ###################### 차원 한번 줄임 ######################\n",
    "        x = x.reshape(x.shape[0],x.shape[1],x.shape[2] * x.shape[3]).permute(2,0,1) # NCHW -> (HW)NC\n",
    "        print('permuted x shape : {}'.format(x.shape))\n",
    "        x = torch.cat([x.mean(dim=0,keepdim=True),x],dim=0) #(HW+1)NC\n",
    "        print('torch cat x shape :',x.shape) #(224 * 224 +1 , 1, 3)\n",
    "        x = x + self.positional_embedding[:,None, :].to(x.dtype) # (HW+1)NC\n",
    "        print(\"added with positional_embedding shape :\",x.shape)\n",
    "        ## multi head attention 수행\n",
    "        x, _ = F.multi_head_attention_forward(\n",
    "            query = x, key = x, value = x, \n",
    "            \n",
    "            embed_dim_to_check = x.shape[-1],\n",
    "            num_heads = self.num_heads, \n",
    "\n",
    "            q_proj_weight = self.q_proj.weight, #nn.Linear(embed_dim,embed_dim) 의 weight\n",
    "            k_proj_weight = self.k_proj.weight, #Linear \n",
    "            v_proj_weight = self.v_proj.weight, \n",
    "            in_proj_weight = None, \n",
    "            in_proj_bias = torch.cat([self.q_proj.bias, self.k_proj.bias, self.v_proj.bias]), \n",
    "            bias_k = None,\n",
    "            bias_v = None, \n",
    "            add_zero_attn = False, \n",
    "            dropout_p= 0, \n",
    "            out_proj_weight=self.c_proj.weight, \n",
    "            out_proj_bias= self.c_proj.bias, \n",
    "            use_separate_proj_weight=True, \n",
    "            training = self.training, \n",
    "            need_weights = False\n",
    "        )\n",
    "\n",
    "        return x[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LayerNorm(nn.LayerNorm):\n",
    "\n",
    "    def forward(self,x : torch.Tensor):\n",
    "        orig_type = x.dtype \n",
    "        \n",
    "        ret = super().forward(x.type(torch.float32))\n",
    "        return ret.type(orig_type)\n",
    "\n",
    "class QuickGELU(nn.Module):\n",
    "    def forward(self,x:torch.Tensor):\n",
    "        return x * torch.sigmoid(1.702 * x)\n",
    "\n",
    "class ResidualAttentionBlock(nn.Module):\n",
    "    # d_model이 Transformer에서 넘어온 width와 같음.\n",
    "    # # d_model == width  \n",
    "    def __init__(self, d_model : int, n_head : int, attn_mask = None):\n",
    "        super(ResidualAttentionBlock,self).__init__()\n",
    "        \n",
    "        ##### attention Block\n",
    "        self.attn = nn.MultiheadAttention(d_model,n_head)\n",
    "        self.ln_1 = LayerNorm(d_model)\n",
    "        self.mlp = nn.Sequential(OrderedDict([\n",
    "            (\"c_fc\",nn.Linear(d_model,d_model*4)),\n",
    "            (\"gelu\",QuickGELU()),\n",
    "            (\"c_proj\",nn.Linear(d_model * 4, d_model))\n",
    "        ]))\n",
    "        self.ln_2 = LayerNorm(d_model)\n",
    "        self.attn_mask = attn_mask \n",
    "    \n",
    "    def attention(self,x:torch.Tensor):\n",
    "        attn_mask_ = self.attn_mask \n",
    "        if self.attn_mask is not None and hasattr(self.attn_mask, '__call__'):\n",
    "            attn_mask_ = self.attn_mask(x.size(0))\n",
    "        attn_mask_ = attn_mask_.to(dtype=x.dtype,device = x.device) if attn_mask_ is not None else None \n",
    "        return self.attn(x,x,x,need_weights=False, attn_mask = attn_mask_)[0]\n",
    "    \n",
    "    def forward(self,x_tuple : tuple):\n",
    "        x,video_frame = x_tuple  \n",
    "        #print(\"Residualattention video_frame :\",video_frame)\n",
    "        x = x + self.attention(self.ln_1(x))\n",
    "        x = x + self.mlp(self.ln_2(x))\n",
    "        return (x,video_frame)\n",
    "\n",
    "class Transformer(nn.Module):\n",
    "    def __init__(self, width :int, layers :int , heads :int, attn_mask = None):\n",
    "        super(Transformer,self).__init__()\n",
    "        self.width = width \n",
    "        self.layers = layers \n",
    "        # layer의 숫자만큼 Residualattention을 진행\n",
    "        self.resblocks = nn.Sequential(*[ResidualAttentionBlock(width,heads,attn_mask) for _ in range(layers)])\n",
    "\n",
    "    def forward(self, x: torch.Tensor, video_frame = -1):\n",
    "        return self.resblocks((x,video_frame))[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "7. CLIP 해부 \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "7-1. CLIP 초기화에 필요한 Vision parameter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "vision_layers = len([k for k in clip_state_dict.keys() if k.startswith(\"visual.\") and k.endswith(\".attn.in_proj_weight\")])\n",
    "vision_width = clip_state_dict[\"visual.conv1.weight\"].shape[0]\n",
    "vision_patch_size = clip_state_dict[\"visual.conv1.weight\"].shape[-1]\n",
    "grid_size = round((clip_state_dict[\"visual.positional_embedding\"].shape[0] - 1) ** 0.5)\n",
    "image_resolution = vision_patch_size * grid_size # 32 x 7 \n",
    "vision_heads = vision_width //64\n",
    "linear_patch = '3d'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([768, 3, 32, 32])\n",
      "32\n"
     ]
    }
   ],
   "source": [
    "print(clip_state_dict[\"visual.conv1.weight\"].shape)\n",
    "print(clip_state_dict[\"visual.conv1.weight\"].shape[-1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "7-2. CLIP 초기화에 필요한 text parameter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n"
     ]
    }
   ],
   "source": [
    "embed_dim = clip_state_dict[\"text_projection\"].shape[1]\n",
    "context_length = clip_state_dict[\"positional_embedding\"].shape[0]\n",
    "vocab_size = clip_state_dict[\"token_embedding.weight\"].shape[0]\n",
    "transformer_width = clip_state_dict[\"ln_final.weight\"].shape[0]\n",
    "transformer_heads = transformer_width // 64\n",
    "#transformer_layers = len(set(k.split(\".\")[2] for k in clip_state_dict if k.startswith(f\"transformer.resblocks\"))) #12\n",
    "transformer_layers = 12\n",
    "\n",
    "token_embedding = nn.Embedding(vocab_size,transformer_width)\n",
    "positional_embedding = nn.Parameter(torch.empty(context_length,transformer_width))\n",
    "ln_final = LayerNorm(transformer_width)\n",
    "text_projection = nn.Parameter(torch.empty(transformer_width,embed_dim))\n",
    "logit_scale = nn.Parameter(torch.ones([]))\n",
    "print(ln_final)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "image resolution : 224\n",
      "vision_patch_size : 32\n",
      "vision_width : 768\n",
      "vision_layers : 12\n",
      "vision_heads : 12\n",
      "vision_heads : 12\n",
      "output_dim : 512\n",
      "linear patch : 3d\n",
      "embed_dim : 512\n",
      "positional_embedding Parameter containing : torch.Size([77, 512])\n",
      "ln_final : LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "text_projection :  torch.Size([512, 512])\n",
      "logit_scale : torch.Size([])\n"
     ]
    }
   ],
   "source": [
    "print(\"image resolution :\",image_resolution)\n",
    "print('vision_patch_size :',vision_patch_size)\n",
    "print('vision_width :',vision_width)\n",
    "print(\"vision_layers :\",vision_layers)\n",
    "print(\"vision_heads :\",vision_heads)\n",
    "print(\"vision_heads :\",vision_heads)\n",
    "print(\"output_dim :\",embed_dim)\n",
    "print(\"linear patch :\",linear_patch)\n",
    "\n",
    "print(\"embed_dim :\",embed_dim)\n",
    "print(\"positional_embedding Parameter containing :\",positional_embedding.shape)\n",
    "print(\"ln_final :\",ln_final)\n",
    "print(\"text_projection : \",text_projection.shape)\n",
    "print(\"logit_scale :\",logit_scale.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "8. VisualTransformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VisualTransformer(nn.Module):\n",
    "    def __init__(self,input_resolution : int, patch_size : int, width : int, layers : int, heads : int, output_dim : int, linear_patch : str = '3d',):\n",
    "        super(VisualTransformer,self).__init__()\n",
    "        self.input_resolution = input_resolution \n",
    "        self.output_dim = output_dim \n",
    "\n",
    "        ##### 2D일 때에는 Conv1d\n",
    "        self.conv1 = nn.Conv2d(in_channels=3, out_channels = width, kernel_size = patch_size,stride=patch_size,bias=False)\n",
    "\n",
    "        scale = width ** -0.5\n",
    "        self.class_embedding = nn.Parameter(scale * torch.randn(width))\n",
    "        self.positional_embedding = nn.Parameter(scale * torch.randn((input_resolution // patch_size)**2 +1, width)) \n",
    "        self.ln_pre = LayerNorm(width)\n",
    "\n",
    "\n",
    "        #Transformer 인자 1 : width / 인자 2 : layers / 인자 3 : heads \n",
    "        self.transformer = Transformer(width,layers,heads)\n",
    "\n",
    "\n",
    "        self.ln_post = LayerNorm(width)\n",
    "        self.proj = nn.Parameter(scale * torch.randn(width,output_dim))   \n",
    "\n",
    "        assert linear_patch in ['2d','3d']\n",
    "        self.linear_patch = linear_patch \n",
    "        if self.linear_patch == '3d':\n",
    "            #### 3D일 때에는 Conv2d\n",
    "            self.conv2 = nn.Conv3d(in_channels=3, out_channels=width, kernel_size = (3,patch_size,patch_size), \n",
    "            stride = (1,patch_size,patch_size),padding = (1,0,0),bias= False)\n",
    "    # Task 2 !!! \n",
    "    # 다시\n",
    "    def forward(self, x, video_frame=-1):\n",
    "            ####Conv3D AE 20220526 #####\n",
    "            #x_3d shape : \n",
    "            x_3d = x.reshape(-1, video_frame, x.shape[-3], x.shape[-2], x.shape[-1])\n",
    "            print(\"1111 x_3d shape :\",x_3d.shape)\n",
    "            if self.linear_patch == '3d':\n",
    "                assert video_frame != -1\n",
    "                x_3d = x.reshape(-1, video_frame, x.shape[-3], x.shape[-2], x.shape[-1])\n",
    "                print(\"2222 x_3d shape :\",x_3d.shape)\n",
    "                x_3d = x_3d.permute(0, 2, 1, 3, 4) #\n",
    "                print(\"3333 x_3d shape :\",x_3d.shape)\n",
    "                #x_3d = x_3d.permute()\n",
    "                # print(\"<<<<x_3d type : {} >>>>\".format(x_3d.dtype))\n",
    "                # x_3d = x_3d.double()\n",
    "                # print(\"<<<<x_3d type : {} >>>>\".format(x_3d.dtype))\n",
    "                #x_3d = x_3d.dtype(x_3d)\n",
    "                x_3d = self.conv2(x_3d)     # shape = [*, width, frame, grid, grid]\n",
    "                print(\"4444 x_3d shape :\",x_3d.shape)\n",
    "                x_3d = x_3d.permute(0, 2, 1, 3, 4)      # shape = [*, frame, width, grid, grid]\n",
    "                print(\"5555 x_3d shape :\",x_3d.shape)\n",
    "                x = x_3d.reshape(-1, x_3d.shape[-3], x_3d.shape[-2], x_3d.shape[-1]).contiguous() # shape = [*, width, grid, grid]\n",
    "                print(\"666 x_3d shape :\",x_3d.shape)\n",
    "            else:\n",
    "                x = self.conv1(x)  # shape = [*, width, grid, grid]\n",
    "\n",
    "            x = x.reshape(x.shape[0], x.shape[1], -1)  # shape = [*, width, grid ** 2]\n",
    "            x = x.permute(0, 2, 1)  # shape = [*, grid ** 2, width]\n",
    "\n",
    "            #################################### 20220607 여기까지는 구현 성공함 ##################################\n",
    "            x = torch.cat([self.class_embedding.to(x.dtype) + torch.zeros(x.shape[0], 1, x.shape[-1], dtype=x.dtype, device=x.device), x], dim=1)  # shape = [*, grid ** 2 + 1, width]\n",
    "            x = x + self.positional_embedding.to(x.dtype) \n",
    "            x = self.ln_pre(x)\n",
    "            #print(\"<<<< x shape : >>>>>\",x.shape)\n",
    "            ################# TAE!!! #################### \n",
    "            x = x.permute(1, 0, 2)  # NLD -> LND\n",
    "            ############### 원래 기존으로 가 #######################\n",
    "            x = self.transformer(x, video_frame=video_frame)\n",
    "            x = x.permute(1, 0, 2)  # LND -> NLD\n",
    "\n",
    "            # Move the three lines below to `encode_image` for entire hidden sequence\n",
    "            # x = self.ln_post(x[:, 0, :])\n",
    "            # if self.proj is not None:\n",
    "            #     x = x @ self.proj\n",
    "\n",
    "            return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AUTOENCODER(nn.Module):\n",
    "    def __init__(self, channel, width : int, height : int, linear_patch='3d'):\n",
    "        super(AUTOENCODER,self).__init__()\n",
    "        self.width  = width \n",
    "        self.height = height \n",
    "        assert linear_patch in ['2d','3d']\n",
    "        self.linear_patch = linear_patch\n",
    "\n",
    "        def EncoderLayer(in_channel,out_channel,pooling_size=None):\n",
    "            network=[] \n",
    "\n",
    "            network += [nn.Conv3d(in_channel, out_channel, kernel_size=3, padding=1)]\n",
    "            network += [nn.BatchNorm3d(out_channel)]\n",
    "            network += [nn.LeakyReLU()]\n",
    "            if pooling_size is not None:\n",
    "                network += [nn.MaxPool3d(kernel_size=pooling_size)]\n",
    "            network = nn.Sequential(*network)\n",
    "            return network\n",
    "\n",
    "        def DecoderLayer(in_channel,out_channel,size=None):\n",
    "            network = [] \n",
    "            if size is None:\n",
    "                network +=[nn.Conv3d(in_channel,out_channel,kernel_size=3,padding=1)]\n",
    "            else:\n",
    "                network +=[nn.ConvTranspose3d(in_channel,out_channel,kernel_size=size,padding=0,stride=size)]\n",
    "            network +=[nn.BatchNorm3d(out_channel)]\n",
    "            network += [nn.LeakyReLU()]\n",
    "            network = nn.Sequential(*network)\n",
    "            return network\n",
    "        \n",
    "        self.encoder1 = EncoderLayer(channel, 32, pooling_size=2)\n",
    "        \n",
    "        self.encoder2 = EncoderLayer(32, 48, pooling_size=2)\n",
    "        self.encoder3 = EncoderLayer(48, 64, pooling_size=2)\n",
    "        self.encoder4 = EncoderLayer(64, 128, pooling_size=2)\n",
    "\n",
    "        self.decoder1 = DecoderLayer(128, 64, size=2)\n",
    "        self.decoder2 = DecoderLayer(64, 48, size=2)\n",
    "        self.decoder3 = DecoderLayer(48, 32, size=2)\n",
    "        self.decoder4 = DecoderLayer(32, channel, size=2)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "        # case 20220607 : self.encoder = EncoderLayer(48, 768) & not used conv2\n",
    "    \n",
    "    def forward(self, x):\n",
    "        encoded = self.encoder4(self.encoder3(self.encoder2(self.encoder1(x))))\n",
    "        decoded = self.decoder4(self.decoder3(self.decoder2(self.decoder1(encoded))))\n",
    "        decoded = self.sigmoid(decoded)\n",
    "        return encoded, decoded\n",
    "        \n",
    "\n",
    "class VAeT(nn.Module):\n",
    "    def __init__(self,input_resolution : int, patch_size : int, width : int, layers : int, heads : int, output_dim : int, linear_patch :str = '2d'):\n",
    "        super(VAeT, self).__init__()\n",
    "\n",
    "        #### channel 값은 3으로 무조건 고정 시킴 \n",
    "        self.AE = AUTOENCODER(channel=3,width=image_resolution,height=image_resolution)\n",
    "        \n",
    "        self.input_resolution = input_resolution\n",
    "        self.output_dim = output_dim \n",
    "\n",
    "        ### 2d일 때에는 Conv1d \n",
    "        self.conv1 = nn.Conv2d(in_channels=3,out_channels=width,kernel_size = patch_size,stride=patch_size, bias=False)\n",
    "\n",
    "        scale = width ** -0.5 \n",
    "        self.class_embedding = nn.Parameter(scale * torch.randn(width))\n",
    "        self.positional_embedding = nn.Parameter(scale * torch.randn(26, width))\n",
    "        self.ln_pre = LayerNorm(width)\n",
    "        \n",
    "        \n",
    "        self.transformer = Transformer(width,layers,heads)\n",
    "\n",
    "        self.ln_post = LayerNorm(width)\n",
    "        self.proj = nn.Parameter(scale *torch.randn(width,output_dim))\n",
    "\n",
    "        assert linear_patch in ['2d','3d']\n",
    "        self.linear_patch = linear_patch \n",
    "\n",
    "        if self.linear_patch == '3d':\n",
    "            self.conv2 = nn.Conv3d(in_channels=128,out_channels=vision_width,kernel_size=(4,4,1),\n",
    "                stride = (3,3,1),padding=(1,1,0),bias=False)\n",
    "\n",
    "    def forward(self, x:torch.Tensor,video_frame=-1):\n",
    "        # x shape : # 32, 1 16 1 3 224 224\n",
    "        # Batch, Pair, Frames, ts, CH, H, W\n",
    "        video_frame = x.shape[2] # video frame\n",
    "\n",
    "        #x = x.permute(0,1,3,4,5,6,2) # 1, 1 1 3 224 224 16\n",
    "        #x = x.reshape(-1, x.shape[-4],x.shape[-3], x.shape[-2], video_frame)\n",
    "        encoded, decoded = self.AE(x)\n",
    "        x = self.conv2(encoded) # result :\n",
    "        x = x.permute(0,4,1,3,2)\n",
    "        x = x.reshape(-1,x.shape[-3],x.shape[-2]*x.shape[-1]).contiguous()\n",
    "        x = x.permute(0,2,1)\n",
    "        x = torch.cat([self.class_embedding.to(x.dtype) + torch.zeros(x.shape[0], 1, x.shape[-1], dtype=x.dtype, device=x.device), x], dim=1)  # shape = [*, grid ** 2 + 1, width]\n",
    "        x = x + self.positional_embedding.to(x.dtype) \n",
    "        x = self.ln_pre(x)\n",
    "        x = x.permute(1,0,2)\n",
    "        x = self.transformer(x,video_frame=1)\n",
    "        x = x.permute(1,0,2)\n",
    "        return x, decoded"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ViT와 VAeT 객체"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "vaet = VAeT(input_resolution=image_resolution,patch_size=vision_patch_size,width=vision_width,\n",
    "layers = vision_layers,heads=vision_heads,output_dim=embed_dim,linear_patch='3d')\n",
    "\n",
    "vit = VisualTransformer(\n",
    "input_resolution = image_resolution, \n",
    "patch_size = vision_patch_size, \n",
    "width = vision_width, \n",
    "layers = vision_layers,\n",
    "heads= vision_heads,\n",
    "output_dim=embed_dim, \n",
    "linear_patch='3d')\n",
    "\n",
    "@property\n",
    "def dtype(self):\n",
    "    return self.visual.conv1.weight.dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "encoded and VAeT res shape : torch.Size([1, 26, 768])\n",
      "res_encoded type : <class 'torch.Tensor'>\n",
      "decoded shape : torch.Size([1, 3, 224, 224, 16])\n"
     ]
    }
   ],
   "source": [
    "sample_video = torch.randn(1,3,224,224,16) # NLD x channel x width x height x frame \n",
    "# res_encoded,decoded = test.forward(sample_video)\n",
    "\n",
    "res_encoded, decoded = vaet(sample_video)\n",
    "print(\"encoded and VAeT res shape :\",res_encoded.shape)\n",
    "print(\"res_encoded type :\",type(res_encoded))\n",
    "print(\"decoded shape :\",decoded.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Expected 5-dimensional input for 5-dimensional weight [32, 3, 3, 3, 3], but got 7-dimensional input of size [1, 1, 16, 1, 3, 224, 224] instead",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m/home/key2317/CLIP4Clip_original/my_module_clip20220620.ipynb Cell 35'\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2B7b22686f73744e616d65223a223136352e3139342e33342e3136222c2275736572223a226b657932333137222c22706f7274223a373732327d/home/key2317/CLIP4Clip_original/my_module_clip20220620.ipynb#ch0000034vscode-remote?line=0'>1</a>\u001b[0m vv \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mrandn(\u001b[39m1\u001b[39m,\u001b[39m1\u001b[39m,\u001b[39m16\u001b[39m,\u001b[39m1\u001b[39m,\u001b[39m3\u001b[39m,\u001b[39m224\u001b[39m,\u001b[39m224\u001b[39m)\n\u001b[0;32m----> <a href='vscode-notebook-cell://ssh-remote%2B7b22686f73744e616d65223a223136352e3139342e33342e3136222c2275736572223a226b657932333137222c22706f7274223a373732327d/home/key2317/CLIP4Clip_original/my_module_clip20220620.ipynb#ch0000034vscode-remote?line=1'>2</a>\u001b[0m e,d \u001b[39m=\u001b[39m vaet(vv)\n",
      "File \u001b[0;32m~/anaconda3/envs/CLIP4Clip/lib/python3.8/site-packages/torch/nn/modules/module.py:889\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    887\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_slow_forward(\u001b[39m*\u001b[39m\u001b[39minput\u001b[39m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[1;32m    888\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 889\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mforward(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    890\u001b[0m     \u001b[39m#print(\"result type : {}\".format(type(result)))\u001b[39;00m\n\u001b[1;32m    891\u001b[0m \u001b[39mfor\u001b[39;00m hook \u001b[39min\u001b[39;00m itertools\u001b[39m.\u001b[39mchain(\n\u001b[1;32m    892\u001b[0m         _global_forward_hooks\u001b[39m.\u001b[39mvalues(),\n\u001b[1;32m    893\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks\u001b[39m.\u001b[39mvalues()):\n",
      "\u001b[1;32m/home/key2317/CLIP4Clip_original/my_module_clip20220620.ipynb Cell 30'\u001b[0m in \u001b[0;36mVAeT.forward\u001b[0;34m(self, x, video_frame)\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B7b22686f73744e616d65223a223136352e3139342e33342e3136222c2275736572223a226b657932333137222c22706f7274223a373732327d/home/key2317/CLIP4Clip_original/my_module_clip20220620.ipynb#ch0000029vscode-remote?line=85'>86</a>\u001b[0m video_frame \u001b[39m=\u001b[39m x\u001b[39m.\u001b[39mshape[\u001b[39m2\u001b[39m] \u001b[39m# video frame\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B7b22686f73744e616d65223a223136352e3139342e33342e3136222c2275736572223a226b657932333137222c22706f7274223a373732327d/home/key2317/CLIP4Clip_original/my_module_clip20220620.ipynb#ch0000029vscode-remote?line=87'>88</a>\u001b[0m \u001b[39m#x = x.permute(0,1,3,4,5,6,2) # 1, 1 1 3 224 224 16\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B7b22686f73744e616d65223a223136352e3139342e33342e3136222c2275736572223a226b657932333137222c22706f7274223a373732327d/home/key2317/CLIP4Clip_original/my_module_clip20220620.ipynb#ch0000029vscode-remote?line=88'>89</a>\u001b[0m \u001b[39m#x = x.reshape(-1, x.shape[-4],x.shape[-3], x.shape[-2], video_frame)\u001b[39;00m\n\u001b[0;32m---> <a href='vscode-notebook-cell://ssh-remote%2B7b22686f73744e616d65223a223136352e3139342e33342e3136222c2275736572223a226b657932333137222c22706f7274223a373732327d/home/key2317/CLIP4Clip_original/my_module_clip20220620.ipynb#ch0000029vscode-remote?line=89'>90</a>\u001b[0m encoded, decoded \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mAE(x)\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B7b22686f73744e616d65223a223136352e3139342e33342e3136222c2275736572223a226b657932333137222c22706f7274223a373732327d/home/key2317/CLIP4Clip_original/my_module_clip20220620.ipynb#ch0000029vscode-remote?line=90'>91</a>\u001b[0m x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mconv2(encoded) \u001b[39m# result :\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B7b22686f73744e616d65223a223136352e3139342e33342e3136222c2275736572223a226b657932333137222c22706f7274223a373732327d/home/key2317/CLIP4Clip_original/my_module_clip20220620.ipynb#ch0000029vscode-remote?line=91'>92</a>\u001b[0m x \u001b[39m=\u001b[39m x\u001b[39m.\u001b[39mpermute(\u001b[39m0\u001b[39m,\u001b[39m4\u001b[39m,\u001b[39m1\u001b[39m,\u001b[39m3\u001b[39m,\u001b[39m2\u001b[39m)\n",
      "File \u001b[0;32m~/anaconda3/envs/CLIP4Clip/lib/python3.8/site-packages/torch/nn/modules/module.py:889\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    887\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_slow_forward(\u001b[39m*\u001b[39m\u001b[39minput\u001b[39m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[1;32m    888\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 889\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mforward(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    890\u001b[0m     \u001b[39m#print(\"result type : {}\".format(type(result)))\u001b[39;00m\n\u001b[1;32m    891\u001b[0m \u001b[39mfor\u001b[39;00m hook \u001b[39min\u001b[39;00m itertools\u001b[39m.\u001b[39mchain(\n\u001b[1;32m    892\u001b[0m         _global_forward_hooks\u001b[39m.\u001b[39mvalues(),\n\u001b[1;32m    893\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks\u001b[39m.\u001b[39mvalues()):\n",
      "\u001b[1;32m/home/key2317/CLIP4Clip_original/my_module_clip20220620.ipynb Cell 30'\u001b[0m in \u001b[0;36mAUTOENCODER.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B7b22686f73744e616d65223a223136352e3139342e33342e3136222c2275736572223a226b657932333137222c22706f7274223a373732327d/home/key2317/CLIP4Clip_original/my_module_clip20220620.ipynb#ch0000029vscode-remote?line=44'>45</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, x):\n\u001b[0;32m---> <a href='vscode-notebook-cell://ssh-remote%2B7b22686f73744e616d65223a223136352e3139342e33342e3136222c2275736572223a226b657932333137222c22706f7274223a373732327d/home/key2317/CLIP4Clip_original/my_module_clip20220620.ipynb#ch0000029vscode-remote?line=45'>46</a>\u001b[0m     encoded \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mencoder4(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mencoder3(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mencoder2(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mencoder1(x))))\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B7b22686f73744e616d65223a223136352e3139342e33342e3136222c2275736572223a226b657932333137222c22706f7274223a373732327d/home/key2317/CLIP4Clip_original/my_module_clip20220620.ipynb#ch0000029vscode-remote?line=46'>47</a>\u001b[0m     decoded \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdecoder4(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdecoder3(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdecoder2(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdecoder1(encoded))))\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B7b22686f73744e616d65223a223136352e3139342e33342e3136222c2275736572223a226b657932333137222c22706f7274223a373732327d/home/key2317/CLIP4Clip_original/my_module_clip20220620.ipynb#ch0000029vscode-remote?line=47'>48</a>\u001b[0m     decoded \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39msigmoid(decoded)\n",
      "File \u001b[0;32m~/anaconda3/envs/CLIP4Clip/lib/python3.8/site-packages/torch/nn/modules/module.py:889\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    887\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_slow_forward(\u001b[39m*\u001b[39m\u001b[39minput\u001b[39m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[1;32m    888\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 889\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mforward(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    890\u001b[0m     \u001b[39m#print(\"result type : {}\".format(type(result)))\u001b[39;00m\n\u001b[1;32m    891\u001b[0m \u001b[39mfor\u001b[39;00m hook \u001b[39min\u001b[39;00m itertools\u001b[39m.\u001b[39mchain(\n\u001b[1;32m    892\u001b[0m         _global_forward_hooks\u001b[39m.\u001b[39mvalues(),\n\u001b[1;32m    893\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks\u001b[39m.\u001b[39mvalues()):\n",
      "File \u001b[0;32m~/anaconda3/envs/CLIP4Clip/lib/python3.8/site-packages/torch/nn/modules/container.py:121\u001b[0m, in \u001b[0;36mSequential.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    117\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m):\n\u001b[1;32m    118\u001b[0m     \u001b[39mfor\u001b[39;00m module \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m:\n\u001b[1;32m    119\u001b[0m         \u001b[39m#print(\"<<<< before input type : {} >>>>\".format(type(input)))\u001b[39;00m\n\u001b[1;32m    120\u001b[0m         \u001b[39m#print(\"<<<<input shape : {}>>>>\".format(input))\u001b[39;00m\n\u001b[0;32m--> 121\u001b[0m         \u001b[39minput\u001b[39m \u001b[39m=\u001b[39m module(\u001b[39minput\u001b[39;49m)\n\u001b[1;32m    122\u001b[0m         \u001b[39m#print(\"<<< after input type : {} >>>>\".format(type(input)))\u001b[39;00m\n\u001b[1;32m    123\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39minput\u001b[39m\n",
      "File \u001b[0;32m~/anaconda3/envs/CLIP4Clip/lib/python3.8/site-packages/torch/nn/modules/module.py:889\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    887\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_slow_forward(\u001b[39m*\u001b[39m\u001b[39minput\u001b[39m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[1;32m    888\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 889\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mforward(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    890\u001b[0m     \u001b[39m#print(\"result type : {}\".format(type(result)))\u001b[39;00m\n\u001b[1;32m    891\u001b[0m \u001b[39mfor\u001b[39;00m hook \u001b[39min\u001b[39;00m itertools\u001b[39m.\u001b[39mchain(\n\u001b[1;32m    892\u001b[0m         _global_forward_hooks\u001b[39m.\u001b[39mvalues(),\n\u001b[1;32m    893\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks\u001b[39m.\u001b[39mvalues()):\n",
      "File \u001b[0;32m~/anaconda3/envs/CLIP4Clip/lib/python3.8/site-packages/torch/nn/modules/conv.py:521\u001b[0m, in \u001b[0;36mConv3d.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    517\u001b[0m     \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39mconv3d(F\u001b[39m.\u001b[39mpad(\u001b[39minput\u001b[39m, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_reversed_padding_repeated_twice, mode\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpadding_mode),\n\u001b[1;32m    518\u001b[0m                     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mweight, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbias, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstride, _triple(\u001b[39m0\u001b[39m),\n\u001b[1;32m    519\u001b[0m                     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdilation, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mgroups)\n\u001b[1;32m    520\u001b[0m \u001b[39m#print(\"self.weight type : {} self.bias type : {} self.stride type : {} self.padding type : {}\".format(type(self.weight),type(self.bias),type(self.stride),type(self.padding)))\u001b[39;00m\n\u001b[0;32m--> 521\u001b[0m \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39;49mconv3d(\u001b[39minput\u001b[39;49m, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mweight, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbias, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mstride,\n\u001b[1;32m    522\u001b[0m                 \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mpadding, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdilation, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mgroups)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Expected 5-dimensional input for 5-dimensional weight [32, 3, 3, 3, 3], but got 7-dimensional input of size [1, 1, 16, 1, 3, 224, 224] instead"
     ]
    }
   ],
   "source": [
    "vv = torch.randn(1,1,16,1,3,224,224)\n",
    "e,d = vaet(vv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "10. CLIP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VAeTCLIP(nn.Module):\n",
    "    def __init__(self,\n",
    "                 embed_dim: int,\n",
    "                 # vision\n",
    "                 input_resolution: int,\n",
    "                 vision_layers: Union[Tuple[int, int, int, int], int],\n",
    "                 vision_width: int,\n",
    "                 vision_patch_size: int,\n",
    "                 # text\n",
    "                 context_length: int,\n",
    "                 vocab_size: int,\n",
    "                 transformer_width: int,\n",
    "                 transformer_heads: int,\n",
    "                 transformer_layers: int,\n",
    "                 # vision linear of patch\n",
    "                 #### 항상 3d로 고정\n",
    "                 linear_patch: str = '3d',\n",
    "                 ):\n",
    "        super().__init__()\n",
    "\n",
    "        self.context_length = context_length\n",
    "\n",
    "        if isinstance(vision_layers, (tuple, list)):\n",
    "            vision_heads = vision_width * 32 // 64\n",
    "            self.visual = ModifiedResNet(\n",
    "                layers=vision_layers,\n",
    "                output_dim=embed_dim,\n",
    "                heads=vision_heads,\n",
    "                input_resolution=input_resolution,\n",
    "                width=vision_width\n",
    "            )\n",
    "        else:\n",
    "            vision_heads = vision_width // 64\n",
    "            self.visual = VAeT(input_resolution=image_resolution,\n",
    "                                patch_size=vision_patch_size,\n",
    "                                width=vision_width,\n",
    "                                layers = vision_layers,heads=vision_heads,\n",
    "                                output_dim=embed_dim,linear_patch='3d')\n",
    "\n",
    "        # 텍스트\n",
    "        self.transformer = Transformer(\n",
    "            width=transformer_width,\n",
    "            layers=transformer_layers,\n",
    "            heads=transformer_heads,\n",
    "            attn_mask=self.build_attention_mask\n",
    "        )\n",
    "\n",
    "        self.vocab_size = vocab_size\n",
    "        self.token_embedding = nn.Embedding(vocab_size, transformer_width)\n",
    "        self.positional_embedding = nn.Parameter(torch.empty(self.context_length, transformer_width))\n",
    "        self.ln_final = LayerNorm(transformer_width)\n",
    "\n",
    "        self.text_projection = nn.Parameter(torch.empty(transformer_width, embed_dim))\n",
    "        self.logit_scale = nn.Parameter(torch.ones([]))\n",
    "\n",
    "        self.initialize_parameters()\n",
    "\n",
    "    def initialize_parameters(self):\n",
    "        nn.init.normal_(self.token_embedding.weight, std=0.02)\n",
    "        nn.init.normal_(self.positional_embedding, std=0.01)\n",
    "\n",
    "        if isinstance(self.visual, ModifiedResNet):\n",
    "            if self.visual.attnpool is not None:\n",
    "                std = self.visual.attnpool.c_proj.in_features ** -0.5\n",
    "                nn.init.normal_(self.visual.attnpool.q_proj.weight, std=std)\n",
    "                nn.init.normal_(self.visual.attnpool.k_proj.weight, std=std)\n",
    "                nn.init.normal_(self.visual.attnpool.v_proj.weight, std=std)\n",
    "                nn.init.normal_(self.visual.attnpool.c_proj.weight, std=std)\n",
    "\n",
    "            for resnet_block in [self.visual.layer1, self.visual.layer2, self.visual.layer3, self.visual.layer4]:\n",
    "                for name, param in resnet_block.named_parameters():\n",
    "                    if name.endswith(\"bn3.weight\"):\n",
    "                        nn.init.zeros_(param)\n",
    "\n",
    "        proj_std = (self.transformer.width ** -0.5) * ((2 * self.transformer.layers) ** -0.5)\n",
    "        attn_std = self.transformer.width ** -0.5\n",
    "        fc_std = (2 * self.transformer.width) ** -0.5\n",
    "        for block in self.transformer.resblocks:\n",
    "            nn.init.normal_(block.attn.in_proj_weight, std=attn_std)\n",
    "            nn.init.normal_(block.attn.out_proj.weight, std=proj_std)\n",
    "            nn.init.normal_(block.mlp.c_fc.weight, std=fc_std)\n",
    "            nn.init.normal_(block.mlp.c_proj.weight, std=proj_std)\n",
    "\n",
    "        if self.text_projection is not None:\n",
    "            nn.init.normal_(self.text_projection, std=self.transformer.width ** -0.5)\n",
    "\n",
    "    @staticmethod\n",
    "    def get_config(pretrained_clip_name=\"ViT-B/32\"):\n",
    "        model_path = os.path.join(os.path.dirname(os.path.abspath(__file__)), \"ViT-B-32.pt\")\n",
    "        if pretrained_clip_name in _MODELS and pretrained_clip_name in _PT_NAME:\n",
    "            model_path = os.path.join(os.path.dirname(os.path.abspath(__file__)), _PT_NAME[pretrained_clip_name])\n",
    "\n",
    "        if pretrained_clip_name in [\"ViT-B/32\", \"ViT-B/16\"] and os.path.exists(model_path):\n",
    "            pass\n",
    "        else:\n",
    "            if pretrained_clip_name in _MODELS:\n",
    "                model_path = _download(_MODELS[pretrained_clip_name])\n",
    "            elif os.path.isfile(pretrained_clip_name):\n",
    "                model_path = pretrained_clip_name\n",
    "            else:\n",
    "                raise RuntimeError(f\"Model {pretrained_clip_name} not found; available models = {available_models()}\")\n",
    "\n",
    "        try:\n",
    "            # loading JIT archive\n",
    "            model = torch.jit.load(model_path, map_location=\"cpu\").eval()\n",
    "            state_dict = model.state_dict()\n",
    "        except RuntimeError:\n",
    "            state_dict = torch.load(model_path, map_location=\"cpu\")\n",
    "\n",
    "        return state_dict\n",
    "\n",
    "    def build_attention_mask(self, context_length):\n",
    "        # lazily create causal attention mask, with full attention between the vision tokens\n",
    "        # pytorch uses additive attention mask; fill with -inf\n",
    "        mask = torch.zeros(context_length, context_length)\n",
    "        mask.fill_(float(\"-inf\"))\n",
    "        mask.triu_(1)  # zero out the lower diagonal\n",
    "        return mask\n",
    "\n",
    "    @property\n",
    "    def dtype(self):\n",
    "        return self.visual.conv1.weight.dtype\n",
    "    \n",
    "    # input : video(4D) video_frame : bs * ts\n",
    "    def encode_image(self, image, return_hidden=False, video_frame=-1):\n",
    "\n",
    "        hidden = self.visual(image.type(self.dtype), video_frame=video_frame)\n",
    "        hidden = self.visual.ln_post(hidden) @ self.visual.proj\n",
    "\n",
    "        x = hidden[:, 0, :]\n",
    "\n",
    "        if return_hidden:\n",
    "            return x, hidden\n",
    "\n",
    "        return x\n",
    "\n",
    "    def encode_text(self, text, return_hidden=False):\n",
    "        x = self.token_embedding(text).type(self.dtype)  # [batch_size, n_ctx, d_model]\n",
    "\n",
    "        pos_emd = self.positional_embedding[:x.size(1), :].type(self.dtype)\n",
    "        x = x + pos_emd\n",
    "        x = x.permute(1, 0, 2)  # NLD -> LND\n",
    "        x = self.transformer(x)\n",
    "        x = x.permute(1, 0, 2)  # LND -> NLD\n",
    "\n",
    "        hidden = self.ln_final(x).type(self.dtype) @ self.text_projection\n",
    "\n",
    "        # x.shape = [batch_size, n_ctx, transformer.width]\n",
    "        # take features from the eot embedding (eot_token is the highest number in each sequence)\n",
    "        x = hidden[torch.arange(hidden.shape[0]), text.argmax(dim=-1)]\n",
    "\n",
    "        if return_hidden:\n",
    "            return x, hidden\n",
    "\n",
    "        return x\n",
    "\n",
    "    def forward(self, image, text):\n",
    "        image_features = self.encode_image(input)\n",
    "        text_features = self.encode_text(text)\n",
    "\n",
    "        # normalized features\n",
    "        image_features = image_features / image_features.norm(dim=-1, keepdim=True)\n",
    "        text_features = text_features / text_features.norm(dim=-1, keepdim=True)\n",
    "\n",
    "        # cosine similarity as logits\n",
    "        logit_scale = self.logit_scale.exp()\n",
    "        logits_per_image = logit_scale * image_features @ text_features.t()\n",
    "        logits_per_text = logit_scale * text_features @ image_features.t()\n",
    "\n",
    "        # shape = [global_batch_size, global_batch_size]\n",
    "        return logits_per_image, logits_per_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CLIP(nn.Module):\n",
    "    def __init__(self,\n",
    "                 embed_dim: int,\n",
    "                 # vision\n",
    "                 input_resolution: int,\n",
    "                 vision_layers: Union[Tuple[int, int, int, int], int],\n",
    "                 vision_width: int,\n",
    "                 vision_patch_size: int,\n",
    "                 # text\n",
    "                 context_length: int,\n",
    "                 vocab_size: int,\n",
    "                 transformer_width: int,\n",
    "                 transformer_heads: int,\n",
    "                 transformer_layers: int,\n",
    "                 # vision linear of patch\n",
    "                 linear_patch: str = '3d',\n",
    "                 ):\n",
    "        super().__init__()\n",
    "\n",
    "        self.context_length = context_length\n",
    "\n",
    "        if isinstance(vision_layers, (tuple, list)):\n",
    "            vision_heads = vision_width * 32 // 64\n",
    "            self.visual = ModifiedResNet(\n",
    "                layers=vision_layers,\n",
    "                output_dim=embed_dim,\n",
    "                heads=vision_heads,\n",
    "                input_resolution=input_resolution,\n",
    "                width=vision_width\n",
    "            )\n",
    "        else:\n",
    "            vision_heads = vision_width // 64\n",
    "            self.visual = VisualTransformer(\n",
    "                input_resolution=input_resolution,\n",
    "                patch_size=vision_patch_size,\n",
    "                width=vision_width,\n",
    "                layers=vision_layers,\n",
    "                heads=vision_heads,\n",
    "                output_dim=embed_dim,\n",
    "                linear_patch=linear_patch\n",
    "            )\n",
    "\n",
    "        self.transformer = Transformer(\n",
    "            width=transformer_width,\n",
    "            layers=transformer_layers,\n",
    "            heads=transformer_heads,\n",
    "            attn_mask=self.build_attention_mask\n",
    "        )\n",
    "\n",
    "        self.vocab_size = vocab_size\n",
    "        self.token_embedding = nn.Embedding(vocab_size, transformer_width)\n",
    "        self.positional_embedding = nn.Parameter(torch.empty(self.context_length, transformer_width))\n",
    "        self.ln_final = LayerNorm(transformer_width)\n",
    "\n",
    "        self.text_projection = nn.Parameter(torch.empty(transformer_width, embed_dim))\n",
    "        self.logit_scale = nn.Parameter(torch.ones([]))\n",
    "\n",
    "        self.initialize_parameters()\n",
    "\n",
    "    def initialize_parameters(self):\n",
    "        nn.init.normal_(self.token_embedding.weight, std=0.02)\n",
    "        nn.init.normal_(self.positional_embedding, std=0.01)\n",
    "\n",
    "        if isinstance(self.visual, ModifiedResNet):\n",
    "            if self.visual.attnpool is not None:\n",
    "                std = self.visual.attnpool.c_proj.in_features ** -0.5\n",
    "                nn.init.normal_(self.visual.attnpool.q_proj.weight, std=std)\n",
    "                nn.init.normal_(self.visual.attnpool.k_proj.weight, std=std)\n",
    "                nn.init.normal_(self.visual.attnpool.v_proj.weight, std=std)\n",
    "                nn.init.normal_(self.visual.attnpool.c_proj.weight, std=std)\n",
    "\n",
    "            for resnet_block in [self.visual.layer1, self.visual.layer2, self.visual.layer3, self.visual.layer4]:\n",
    "                for name, param in resnet_block.named_parameters():\n",
    "                    if name.endswith(\"bn3.weight\"):\n",
    "                        nn.init.zeros_(param)\n",
    "\n",
    "        proj_std = (self.transformer.width ** -0.5) * ((2 * self.transformer.layers) ** -0.5)\n",
    "        attn_std = self.transformer.width ** -0.5\n",
    "        fc_std = (2 * self.transformer.width) ** -0.5\n",
    "        for block in self.transformer.resblocks:\n",
    "            nn.init.normal_(block.attn.in_proj_weight, std=attn_std)\n",
    "            nn.init.normal_(block.attn.out_proj.weight, std=proj_std)\n",
    "            nn.init.normal_(block.mlp.c_fc.weight, std=fc_std)\n",
    "            nn.init.normal_(block.mlp.c_proj.weight, std=proj_std)\n",
    "\n",
    "        if self.text_projection is not None:\n",
    "            nn.init.normal_(self.text_projection, std=self.transformer.width ** -0.5)\n",
    "\n",
    "    @staticmethod\n",
    "    def get_config(pretrained_clip_name=\"ViT-B/32\"):\n",
    "        model_path = os.path.join(os.path.dirname(os.path.abspath(__file__)), \"ViT-B-32.pt\")\n",
    "        if pretrained_clip_name in _MODELS and pretrained_clip_name in _PT_NAME:\n",
    "            model_path = os.path.join(os.path.dirname(os.path.abspath(__file__)), _PT_NAME[pretrained_clip_name])\n",
    "\n",
    "        if pretrained_clip_name in [\"ViT-B/32\", \"ViT-B/16\"] and os.path.exists(model_path):\n",
    "            pass\n",
    "        else:\n",
    "            if pretrained_clip_name in _MODELS:\n",
    "                model_path = _download(_MODELS[pretrained_clip_name])\n",
    "            elif os.path.isfile(pretrained_clip_name):\n",
    "                model_path = pretrained_clip_name\n",
    "            else:\n",
    "                raise RuntimeError(f\"Model {pretrained_clip_name} not found; available models = {available_models()}\")\n",
    "\n",
    "        try:\n",
    "            # loading JIT archive\n",
    "            model = torch.jit.load(model_path, map_location=\"cpu\").eval()\n",
    "            state_dict = model.state_dict()\n",
    "        except RuntimeError:\n",
    "            state_dict = torch.load(model_path, map_location=\"cpu\")\n",
    "\n",
    "        return state_dict\n",
    "\n",
    "    def build_attention_mask(self, context_length):\n",
    "        # lazily create causal attention mask, with full attention between the vision tokens\n",
    "        # pytorch uses additive attention mask; fill with -inf\n",
    "        mask = torch.zeros(context_length, context_length)\n",
    "        mask.fill_(float(\"-inf\"))\n",
    "        mask.triu_(1)  # zero out the lower diagonal\n",
    "        return mask\n",
    "\n",
    "    @property\n",
    "    def dtype(self):\n",
    "        return self.visual.conv1.weight.dtype\n",
    "    \n",
    "    # input : video(4D) video_frame : bs * ts\n",
    "    def encode_image(self, image, return_hidden=False, video_frame=-1):\n",
    "\n",
    "        hidden = self.visual(image.type(self.dtype), video_frame=video_frame)\n",
    "        hidden = self.visual.ln_post(hidden) @ self.visual.proj\n",
    "\n",
    "        x = hidden[:, 0, :]\n",
    "\n",
    "        if return_hidden:\n",
    "            return x, hidden\n",
    "\n",
    "        return x\n",
    "\n",
    "    def encode_text(self, text, return_hidden=False):\n",
    "        x = self.token_embedding(text).type(self.dtype)  # [batch_size, n_ctx, d_model]\n",
    "\n",
    "        pos_emd = self.positional_embedding[:x.size(1), :].type(self.dtype)\n",
    "        x = x + pos_emd\n",
    "        x = x.permute(1, 0, 2)  # NLD -> LND\n",
    "        x = self.transformer(x)\n",
    "        x = x.permute(1, 0, 2)  # LND -> NLD\n",
    "\n",
    "        hidden = self.ln_final(x).type(self.dtype) @ self.text_projection\n",
    "\n",
    "        # x.shape = [batch_size, n_ctx, transformer.width]\n",
    "        # take features from the eot embedding (eot_token is the highest number in each sequence)\n",
    "        x = hidden[torch.arange(hidden.shape[0]), text.argmax(dim=-1)]\n",
    "\n",
    "        if return_hidden:\n",
    "            return x, hidden\n",
    "\n",
    "        return x\n",
    "\n",
    "    def forward(self, image, text):\n",
    "        image_features = self.encode_image(image)\n",
    "        text_features = self.encode_text(text)\n",
    "\n",
    "        # normalized features\n",
    "        image_features = image_features / image_features.norm(dim=-1, keepdim=True)\n",
    "        text_features = text_features / text_features.norm(dim=-1, keepdim=True)\n",
    "\n",
    "        # cosine similarity as logits\n",
    "        logit_scale = self.logit_scale.exp()\n",
    "        logits_per_image = logit_scale * image_features @ text_features.t()\n",
    "        logits_per_text = logit_scale * text_features @ image_features.t()\n",
    "\n",
    "        # shape = [global_batch_size, global_batch_size]\n",
    "        return logits_per_image, logits_per_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vaetclip = VAeTCLIP(\n",
    "    embed_dim,\n",
    "    image_resolution, vision_layers, vision_width, vision_patch_size,\n",
    "    context_length, vocab_size, transformer_width, transformer_heads, transformer_layers\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clip = CLIP(\n",
    "    embed_dim,\n",
    "    image_resolution, vision_layers, vision_width, vision_patch_size,\n",
    "    context_length, vocab_size, transformer_width, transformer_heads, transformer_layers\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "기존의 CLIP4Clip과 개량된 CLIP4Clip 비교"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CLIP4Clip(CLIP4ClipPreTrainedModel):\n",
    "    def __init__(self, cross_config, clip_state_dict, task_config):\n",
    "        super(CLIP4Clip, self).__init__(cross_config)\n",
    "        self.task_config = task_config\n",
    "        self.ignore_video_index = -1\n",
    "\n",
    "        assert self.task_config.max_words + self.task_config.max_frames <= cross_config.max_position_embeddings\n",
    "\n",
    "        self._stage_one = True\n",
    "        self._stage_two = False\n",
    "\n",
    "        show_log(task_config, \"Stage-One:{}, Stage-Two:{}\".format(self._stage_one, self._stage_two))\n",
    "\n",
    "        self.loose_type = False\n",
    "        if self._stage_one and check_attr('loose_type', self.task_config):\n",
    "            self.loose_type = True\n",
    "            show_log(task_config, \"Test retrieval by loose type.\")\n",
    "\n",
    "        # CLIP Encoders: From OpenAI: CLIP [https://github.com/openai/CLIP] ===>\n",
    "        vit = \"visual.proj\" in clip_state_dict\n",
    "        assert vit\n",
    "        if vit:\n",
    "            vision_width = clip_state_dict[\"visual.conv1.weight\"].shape[0]\n",
    "            vision_layers = len(\n",
    "                [k for k in clip_state_dict.keys() if k.startswith(\"visual.\") and k.endswith(\".attn.in_proj_weight\")])\n",
    "            vision_patch_size = clip_state_dict[\"visual.conv1.weight\"].shape[-1]\n",
    "            grid_size = round((clip_state_dict[\"visual.positional_embedding\"].shape[0] - 1) ** 0.5)\n",
    "            image_resolution = vision_patch_size * grid_size\n",
    "        else:\n",
    "            counts: list = [len(set(k.split(\".\")[2] for k in clip_state_dict if k.startswith(f\"visual.layer{b}\"))) for b in\n",
    "                            [1, 2, 3, 4]]\n",
    "            vision_layers = tuple(counts)\n",
    "            vision_width = clip_state_dict[\"visual.layer1.0.conv1.weight\"].shape[0]\n",
    "            output_width = round((clip_state_dict[\"visual.attnpool.positional_embedding\"].shape[0] - 1) ** 0.5)\n",
    "            vision_patch_size = None\n",
    "            assert output_width ** 2 + 1 == clip_state_dict[\"visual.attnpool.positional_embedding\"].shape[0]\n",
    "            image_resolution = output_width * 32\n",
    "\n",
    "        embed_dim = clip_state_dict[\"text_projection\"].shape[1]\n",
    "        context_length = clip_state_dict[\"positional_embedding\"].shape[0]\n",
    "        vocab_size = clip_state_dict[\"token_embedding.weight\"].shape[0]\n",
    "        transformer_width = clip_state_dict[\"ln_final.weight\"].shape[0]\n",
    "        transformer_heads = transformer_width // 64\n",
    "        transformer_layers = len(set(k.split(\".\")[2] for k in clip_state_dict if k.startswith(f\"transformer.resblocks\")))\n",
    "\n",
    "        show_log(task_config, \"\\t embed_dim: {}\".format(embed_dim))\n",
    "        show_log(task_config, \"\\t image_resolution: {}\".format(image_resolution))\n",
    "        show_log(task_config, \"\\t vision_layers: {}\".format(vision_layers))\n",
    "        show_log(task_config, \"\\t vision_width: {}\".format(vision_width))\n",
    "        show_log(task_config, \"\\t vision_patch_size: {}\".format(vision_patch_size))\n",
    "        show_log(task_config, \"\\t context_length: {}\".format(context_length))\n",
    "        show_log(task_config, \"\\t vocab_size: {}\".format(vocab_size))\n",
    "        show_log(task_config, \"\\t transformer_width: {}\".format(transformer_width))\n",
    "        show_log(task_config, \"\\t transformer_heads: {}\".format(transformer_heads))\n",
    "        show_log(task_config, \"\\t transformer_layers: {}\".format(transformer_layers))\n",
    "\n",
    "        self.linear_patch = '3d'\n",
    "        if hasattr(task_config, \"linear_patch\"):\n",
    "            self.linear_patch = task_config.linear_patch\n",
    "            show_log(task_config, \"\\t\\t linear_patch: {}\".format(self.linear_patch))\n",
    "\n",
    "        # use .float() to avoid overflow/underflow from fp16 weight. https://github.com/openai/CLIP/issues/40\n",
    "        cut_top_layer = 0\n",
    "        show_log(task_config, \"\\t cut_top_layer: {}\".format(cut_top_layer))\n",
    "        self.clip = CLIP(\n",
    "            embed_dim,\n",
    "            image_resolution, vision_layers-cut_top_layer, vision_width, vision_patch_size,\n",
    "            context_length, vocab_size, transformer_width, transformer_heads, transformer_layers-cut_top_layer,\n",
    "            linear_patch=self.linear_patch\n",
    "        ).float()\n",
    "\n",
    "        for key in [\"input_resolution\", \"context_length\", \"vocab_size\"]:\n",
    "            if key in clip_state_dict:\n",
    "                del clip_state_dict[key]\n",
    "\n",
    "        convert_weights(self.clip)\n",
    "        # <=== End of CLIP Encoders\n",
    "\n",
    "        self.sim_header = 'meanP'\n",
    "        if hasattr(task_config, \"sim_header\"):\n",
    "            self.sim_header = task_config.sim_header\n",
    "            show_log(task_config, \"\\t sim_header: {}\".format(self.sim_header))\n",
    "        if self.sim_header == \"tightTransf\": assert self.loose_type is False\n",
    "\n",
    "        cross_config.max_position_embeddings = context_length\n",
    "        if self.loose_type is False:\n",
    "            # Cross Encoder ===>\n",
    "            cross_config = update_attr(\"cross_config\", cross_config, \"num_hidden_layers\", self.task_config, \"cross_num_hidden_layers\")\n",
    "            self.cross = CrossModel(cross_config)\n",
    "            # <=== End of Cross Encoder\n",
    "            self.similarity_dense = nn.Linear(cross_config.hidden_size, 1)\n",
    "\n",
    "        if self.sim_header == \"seqLSTM\" or self.sim_header == \"seqTransf\":\n",
    "            self.frame_position_embeddings = nn.Embedding(cross_config.max_position_embeddings, cross_config.hidden_size)\n",
    "        if self.sim_header == \"seqTransf\":\n",
    "            self.transformerClip = TransformerClip(width=transformer_width, layers=self.task_config.cross_num_hidden_layers,\n",
    "                                                   heads=transformer_heads, )\n",
    "        if self.sim_header == \"seqLSTM\":\n",
    "            self.lstm_visual = nn.LSTM(input_size=cross_config.hidden_size, hidden_size=cross_config.hidden_size,\n",
    "                                       batch_first=True, bidirectional=False, num_layers=1)\n",
    "\n",
    "        self.loss_fct = CrossEn()\n",
    "\n",
    "        self.apply(self.init_weights)\n",
    "\n",
    "    # video : 7D\n",
    "    def forward(self, input_ids, token_type_ids, attention_mask, video, video_mask=None):\n",
    "        input_ids = input_ids.view(-1, input_ids.shape[-1])\n",
    "        token_type_ids = token_type_ids.view(-1, token_type_ids.shape[-1])\n",
    "        attention_mask = attention_mask.view(-1, attention_mask.shape[-1])\n",
    "        video_mask = video_mask.view(-1, video_mask.shape[-1])\n",
    "\n",
    "        # T x 3 x H x W\n",
    "        video = torch.as_tensor(video).float()\n",
    "        b, pair, bs, ts, channel, h, w = video.shape\n",
    "        # video : 4D\n",
    "        video = video.view(b * pair * bs * ts, channel, h, w)\n",
    "        video_frame = bs * ts\n",
    "\n",
    "        sequence_output, visual_output = self.get_sequence_visual_output(input_ids, token_type_ids, attention_mask,\n",
    "                                                                         video, video_mask, shaped=True, video_frame=video_frame)\n",
    "\n",
    "        if self.training:\n",
    "            loss = 0.\n",
    "            #seq output : [1 x 512] \n",
    "            #vis output(ours) : [1 x 512]\n",
    "            # decoded output(ours) : [1, 16, 3, 224, 224]\n",
    "            #\n",
    "            sim_matrix, *_tmp = self.get_similarity_logits(sequence_output, visual_output, attention_mask, video_mask,\n",
    "                                                    shaped=True, loose_type=self.loose_type)\n",
    "            sim_loss1 = self.loss_fct(sim_matrix)\n",
    "            sim_loss2 = self.loss_fct(sim_matrix.T)\n",
    "            sim_loss = (sim_loss1 + sim_loss2) / 2\n",
    "            loss += sim_loss\n",
    "\n",
    "            return loss\n",
    "        else:\n",
    "            return None\n",
    "\n",
    "    def get_sequence_output(self, input_ids, token_type_ids, attention_mask, shaped=False):\n",
    "        if shaped is False:\n",
    "            input_ids = input_ids.view(-1, input_ids.shape[-1])\n",
    "            token_type_ids = token_type_ids.view(-1, token_type_ids.shape[-1])\n",
    "            attention_mask = attention_mask.view(-1, attention_mask.shape[-1])\n",
    "\n",
    "        bs_pair = input_ids.size(0)\n",
    "        sequence_hidden = self.clip.encode_text(input_ids).float()\n",
    "        sequence_hidden = sequence_hidden.view(bs_pair, -1, sequence_hidden.size(-1))\n",
    "\n",
    "        return sequence_hidden\n",
    "\n",
    "    def get_visual_output(self, video, video_mask, shaped=False, video_frame=-1):\n",
    "        if shaped is False:\n",
    "            video_mask = video_mask.view(-1, video_mask.shape[-1])\n",
    "            video = torch.as_tensor(video).float()\n",
    "            b, pair, bs, ts, channel, h, w = video.shape\n",
    "            video = video.view(b * pair * bs * ts, channel, h, w)\n",
    "            video_frame = bs * ts\n",
    "        # video : 4D\n",
    "        # video = video.view(b * pair * bs * ts, channel, h, w)\n",
    "        bs_pair = video_mask.size(0) # bs_pair : b * pair * bs * ts\n",
    "\n",
    "        ##################### 20220607 ###########################\n",
    "        visual_hidden = self.clip.encode_image(video, video_frame=video_frame).float()\n",
    "\n",
    "        # Case 1 : clip에 정의 되어 있는 encode_imae를 쓰지 않고, 상속 \n",
    "\n",
    "        # Case 2 : clip을 변경 \n",
    "\n",
    "\n",
    "        visual_hidden = visual_hidden.view(bs_pair, -1, visual_hidden.size(-1))\n",
    "\n",
    "        return visual_hidden\n",
    "    #video 4D\n",
    "    def get_sequence_visual_output(self, input_ids, token_type_ids, attention_mask, video, video_mask, shaped=False, video_frame=-1):\n",
    "        if shaped is False:\n",
    "            input_ids = input_ids.view(-1, input_ids.shape[-1])\n",
    "            token_type_ids = token_type_ids.view(-1, token_type_ids.shape[-1])\n",
    "            attention_mask = attention_mask.view(-1, attention_mask.shape[-1])\n",
    "            video_mask = video_mask.view(-1, video_mask.shape[-1])\n",
    "\n",
    "            video = torch.as_tensor(video).float()\n",
    "            b, pair, bs, ts, channel, h, w = video.shape\n",
    "            video = video.view(b * pair * bs * ts, channel, h, w)\n",
    "            video_frame = bs * ts\n",
    "\n",
    "        sequence_output = self.get_sequence_output(input_ids, token_type_ids, attention_mask, shaped=True)\n",
    "        visual_output = self.get_visual_output(video, video_mask, shaped=True, video_frame=video_frame)\n",
    "\n",
    "        return sequence_output, visual_output # v : [16 x 512]\n",
    "    \n",
    "\n",
    "    #두 서로 다른 모달리티를 더하기 \n",
    "    def _get_cross_output(self, sequence_output, visual_output, attention_mask, video_mask):\n",
    "\n",
    "        concat_features = torch.cat((sequence_output, visual_output), dim=1)  # concatnate tokens and frames\n",
    "        concat_mask = torch.cat((attention_mask, video_mask), dim=1)\n",
    "        text_type_ = torch.zeros_like(attention_mask)\n",
    "        video_type_ = torch.ones_like(video_mask)\n",
    "        concat_type = torch.cat((text_type_, video_type_), dim=1)\n",
    "\n",
    "        cross_layers, pooled_output = self.cross(concat_features, concat_type, concat_mask, output_all_encoded_layers=True)\n",
    "        cross_output = cross_layers[-1]\n",
    "\n",
    "        return cross_output, pooled_output, concat_mask\n",
    "\n",
    "    def _mean_pooling_for_similarity_sequence(self, sequence_output, attention_mask):\n",
    "        attention_mask_un = attention_mask.to(dtype=torch.float).unsqueeze(-1)\n",
    "        attention_mask_un[:, 0, :] = 0.\n",
    "        sequence_output = sequence_output * attention_mask_un\n",
    "        text_out = torch.sum(sequence_output, dim=1) / torch.sum(attention_mask_un, dim=1, dtype=torch.float)\n",
    "        return text_out\n",
    "\n",
    "    def _mean_pooling_for_similarity_visual(self, visual_output, video_mask,):\n",
    "        video_mask_un = video_mask.to(dtype=torch.float).unsqueeze(-1)\n",
    "        visual_output = visual_output * video_mask_un\n",
    "        video_mask_un_sum = torch.sum(video_mask_un, dim=1, dtype=torch.float)\n",
    "        video_mask_un_sum[video_mask_un_sum == 0.] = 1.\n",
    "        video_out = torch.sum(visual_output, dim=1) / video_mask_un_sum\n",
    "        return video_out\n",
    "\n",
    "    def _mean_pooling_for_similarity(self, sequence_output, visual_output, attention_mask, video_mask,):\n",
    "        text_out = self._mean_pooling_for_similarity_sequence(sequence_output, attention_mask)\n",
    "        video_out = self._mean_pooling_for_similarity_visual(visual_output, video_mask)\n",
    "\n",
    "        return text_out, video_out\n",
    "\n",
    "    def _loose_similarity(self, sequence_output, visual_output, attention_mask, video_mask, sim_header=\"meanP\"):\n",
    "        sequence_output, visual_output = sequence_output.contiguous(), visual_output.contiguous()\n",
    "\n",
    "        if sim_header == \"meanP\":\n",
    "            # Default: Parameter-free type\n",
    "            pass\n",
    "        elif sim_header == \"seqLSTM\":\n",
    "            # Sequential type: LSTM\n",
    "            visual_output_original = visual_output\n",
    "            visual_output = pack_padded_sequence(visual_output, torch.sum(video_mask, dim=-1).cpu(),\n",
    "                                                 batch_first=True, enforce_sorted=False)\n",
    "            visual_output, _ = self.lstm_visual(visual_output)\n",
    "            if self.training: self.lstm_visual.flatten_parameters()\n",
    "            visual_output, _ = pad_packed_sequence(visual_output, batch_first=True)\n",
    "            visual_output = torch.cat((visual_output, visual_output_original[:, visual_output.size(1):, ...].contiguous()), dim=1)\n",
    "            visual_output = visual_output + visual_output_original\n",
    "        elif sim_header == \"seqTransf\":\n",
    "            # Sequential type: Transformer Encoder\n",
    "            visual_output_original = visual_output\n",
    "            seq_length = visual_output.size(1)\n",
    "            position_ids = torch.arange(seq_length, dtype=torch.long, device=visual_output.device)\n",
    "            position_ids = position_ids.unsqueeze(0).expand(visual_output.size(0), -1)\n",
    "            frame_position_embeddings = self.frame_position_embeddings(position_ids)\n",
    "            visual_output = visual_output + frame_position_embeddings\n",
    "\n",
    "            extended_video_mask = (1.0 - video_mask.unsqueeze(1)) * -1000000.0\n",
    "            extended_video_mask = extended_video_mask.expand(-1, video_mask.size(1), -1)\n",
    "            visual_output = visual_output.permute(1, 0, 2)  # NLD -> LND\n",
    "            visual_output = self.transformerClip(visual_output, extended_video_mask)\n",
    "            visual_output = visual_output.permute(1, 0, 2)  # LND -> NLD\n",
    "            visual_output = visual_output + visual_output_original\n",
    "\n",
    "        if self.training:\n",
    "            visual_output = allgather(visual_output, self.task_config)\n",
    "            video_mask = allgather(video_mask, self.task_config)\n",
    "            sequence_output = allgather(sequence_output, self.task_config)\n",
    "            torch.distributed.barrier()\n",
    "\n",
    "        visual_output = visual_output / visual_output.norm(dim=-1, keepdim=True)\n",
    "        visual_output = self._mean_pooling_for_similarity_visual(visual_output, video_mask)\n",
    "        visual_output = visual_output / visual_output.norm(dim=-1, keepdim=True)\n",
    "\n",
    "        sequence_output = sequence_output.squeeze(1)\n",
    "        sequence_output = sequence_output / sequence_output.norm(dim=-1, keepdim=True)\n",
    "\n",
    "        logit_scale = self.clip.logit_scale.exp()\n",
    "        retrieve_logits = logit_scale * torch.matmul(sequence_output, visual_output.t())\n",
    "        return retrieve_logits\n",
    "\n",
    "    def _cross_similarity(self, sequence_output, visual_output, attention_mask, video_mask):\n",
    "        sequence_output, visual_output = sequence_output.contiguous(), visual_output.contiguous()\n",
    "\n",
    "        b_text, s_text, h_text = sequence_output.size()\n",
    "        b_visual, s_visual, h_visual = visual_output.size()\n",
    "\n",
    "        retrieve_logits_list = []\n",
    "\n",
    "        step_size = b_text      # set smaller to reduce memory cost\n",
    "        split_size = [step_size] * (b_text // step_size)\n",
    "        release_size = b_text - sum(split_size)\n",
    "        if release_size > 0:\n",
    "            split_size += [release_size]\n",
    "\n",
    "        # due to clip text branch retrun the last hidden\n",
    "        attention_mask = torch.ones(sequence_output.size(0), 1)\\\n",
    "            .to(device=attention_mask.device, dtype=attention_mask.dtype)\n",
    "\n",
    "        sequence_output_splits = torch.split(sequence_output, split_size, dim=0)\n",
    "        attention_mask_splits = torch.split(attention_mask, split_size, dim=0)\n",
    "        for i in range(len(split_size)):\n",
    "            sequence_output_row = sequence_output_splits[i]\n",
    "            attention_mask_row = attention_mask_splits[i]\n",
    "            sequence_output_l = sequence_output_row.unsqueeze(1).repeat(1, b_visual, 1, 1)\n",
    "            sequence_output_l = sequence_output_l.view(-1, s_text, h_text)\n",
    "            attention_mask_l = attention_mask_row.unsqueeze(1).repeat(1, b_visual, 1)\n",
    "            attention_mask_l = attention_mask_l.view(-1, s_text)\n",
    "\n",
    "            step_truth = sequence_output_row.size(0)\n",
    "            visual_output_r = visual_output.unsqueeze(0).repeat(step_truth, 1, 1, 1)\n",
    "            visual_output_r = visual_output_r.view(-1, s_visual, h_visual)\n",
    "            video_mask_r = video_mask.unsqueeze(0).repeat(step_truth, 1, 1)\n",
    "            video_mask_r = video_mask_r.view(-1, s_visual)\n",
    "\n",
    "            cross_output, pooled_output, concat_mask = \\\n",
    "                self._get_cross_output(sequence_output_l, visual_output_r, attention_mask_l, video_mask_r)\n",
    "            retrieve_logits_row = self.similarity_dense(pooled_output).squeeze(-1).view(step_truth, b_visual)\n",
    "\n",
    "            retrieve_logits_list.append(retrieve_logits_row)\n",
    "\n",
    "        retrieve_logits = torch.cat(retrieve_logits_list, dim=0)\n",
    "        return retrieve_logits\n",
    "\n",
    "    def get_similarity_logits(self, sequence_output, visual_output, attention_mask, video_mask, shaped=False, loose_type=False):\n",
    "        if shaped is False:\n",
    "            attention_mask = attention_mask.view(-1, attention_mask.shape[-1])\n",
    "            video_mask = video_mask.view(-1, video_mask.shape[-1])\n",
    "\n",
    "        contrastive_direction = ()\n",
    "        if loose_type:\n",
    "            assert self.sim_header in [\"meanP\", \"seqLSTM\", \"seqTransf\"]\n",
    "            retrieve_logits = self._loose_similarity(sequence_output, visual_output, attention_mask, video_mask, sim_header=self.sim_header)\n",
    "        else:\n",
    "            assert self.sim_header in [\"tightTransf\"]\n",
    "            retrieve_logits = self._cross_similarity(sequence_output, visual_output, attention_mask, video_mask, )\n",
    "\n",
    "        return retrieve_logits, contrastive_direction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VAeTCLIP4Clip(CLIP4ClipPreTrainedModel):\n",
    "    def __init__(self, cross_config, clip_state_dict, task_config):\n",
    "        super(VAeTCLIP4Clip, self).__init__(cross_config)\n",
    "        self.task_config = task_config\n",
    "        self.ignore_video_index = -1\n",
    "\n",
    "        assert self.task_config.max_words + self.task_config.max_frames <= cross_config.max_position_embeddings\n",
    "\n",
    "        self._stage_one = True\n",
    "        self._stage_two = False\n",
    "\n",
    "        show_log(task_config, \"Stage-One:{}, Stage-Two:{}\".format(self._stage_one, self._stage_two))\n",
    "\n",
    "        self.loose_type = False\n",
    "        if self._stage_one and check_attr('loose_type', self.task_config):\n",
    "            self.loose_type = True\n",
    "            show_log(task_config, \"Test retrieval by loose type.\")\n",
    "\n",
    "        # CLIP Encoders: From OpenAI: CLIP [https://github.com/openai/CLIP] ===>\n",
    "        vit = \"visual.proj\" in clip_state_dict\n",
    "        assert vit\n",
    "        if vit:\n",
    "            vision_width = clip_state_dict[\"visual.conv1.weight\"].shape[0]\n",
    "            vision_layers = len(\n",
    "                [k for k in clip_state_dict.keys() if k.startswith(\"visual.\") and k.endswith(\".attn.in_proj_weight\")])\n",
    "            vision_patch_size = clip_state_dict[\"visual.conv1.weight\"].shape[-1]\n",
    "            grid_size = round((clip_state_dict[\"visual.positional_embedding\"].shape[0] - 1) ** 0.5)\n",
    "            image_resolution = vision_patch_size * grid_size\n",
    "        else:\n",
    "            counts: list = [len(set(k.split(\".\")[2] for k in clip_state_dict if k.startswith(f\"visual.layer{b}\"))) for b in\n",
    "                            [1, 2, 3, 4]]\n",
    "            vision_layers = tuple(counts)\n",
    "            vision_width = clip_state_dict[\"visual.layer1.0.conv1.weight\"].shape[0]\n",
    "            output_width = round((clip_state_dict[\"visual.attnpool.positional_embedding\"].shape[0] - 1) ** 0.5)\n",
    "            vision_patch_size = None\n",
    "            assert output_width ** 2 + 1 == clip_state_dict[\"visual.attnpool.positional_embedding\"].shape[0]\n",
    "            image_resolution = output_width * 32\n",
    "\n",
    "        embed_dim = clip_state_dict[\"text_projection\"].shape[1]\n",
    "        context_length = clip_state_dict[\"positional_embedding\"].shape[0]\n",
    "        vocab_size = clip_state_dict[\"token_embedding.weight\"].shape[0]\n",
    "        transformer_width = clip_state_dict[\"ln_final.weight\"].shape[0]\n",
    "        transformer_heads = transformer_width // 64\n",
    "        transformer_layers = len(set(k.split(\".\")[2] for k in clip_state_dict if k.startswith(f\"transformer.resblocks\")))\n",
    "\n",
    "        show_log(task_config, \"\\t embed_dim: {}\".format(embed_dim))\n",
    "        show_log(task_config, \"\\t image_resolution: {}\".format(image_resolution))\n",
    "        show_log(task_config, \"\\t vision_layers: {}\".format(vision_layers))\n",
    "        show_log(task_config, \"\\t vision_width: {}\".format(vision_width))\n",
    "        show_log(task_config, \"\\t vision_patch_size: {}\".format(vision_patch_size))\n",
    "        show_log(task_config, \"\\t context_length: {}\".format(context_length))\n",
    "        show_log(task_config, \"\\t vocab_size: {}\".format(vocab_size))\n",
    "        show_log(task_config, \"\\t transformer_width: {}\".format(transformer_width))\n",
    "        show_log(task_config, \"\\t transformer_heads: {}\".format(transformer_heads))\n",
    "        show_log(task_config, \"\\t transformer_layers: {}\".format(transformer_layers))\n",
    "\n",
    "        self.linear_patch = '3d'\n",
    "        if hasattr(task_config, \"linear_patch\"):\n",
    "            self.linear_patch = task_config.linear_patch\n",
    "            show_log(task_config, \"\\t\\t linear_patch: {}\".format(self.linear_patch))\n",
    "\n",
    "        # use .float() to avoid overflow/underflow from fp16 weight. https://github.com/openai/CLIP/issues/40\n",
    "        cut_top_layer = 0\n",
    "        show_log(task_config, \"\\t cut_top_layer: {}\".format(cut_top_layer))\n",
    "        self.clip = VAeTCLIP(\n",
    "            embed_dim,\n",
    "            image_resolution, vision_layers-cut_top_layer, vision_width, vision_patch_size,\n",
    "            context_length, vocab_size, transformer_width, transformer_heads, transformer_layers-cut_top_layer,\n",
    "            linear_patch=self.linear_patch\n",
    "        ).float()\n",
    "\n",
    "        for key in [\"input_resolution\", \"context_length\", \"vocab_size\"]:\n",
    "            if key in clip_state_dict:\n",
    "                del clip_state_dict[key]\n",
    "\n",
    "        convert_weights(self.clip)\n",
    "        # <=== End of CLIP Encoders\n",
    "\n",
    "        self.sim_header = 'meanP'\n",
    "        if hasattr(task_config, \"sim_header\"):\n",
    "            self.sim_header = task_config.sim_header\n",
    "            show_log(task_config, \"\\t sim_header: {}\".format(self.sim_header))\n",
    "        if self.sim_header == \"tightTransf\": assert self.loose_type is False\n",
    "\n",
    "        cross_config.max_position_embeddings = context_length\n",
    "        if self.loose_type is False:\n",
    "            # Cross Encoder ===>\n",
    "            cross_config = update_attr(\"cross_config\", cross_config, \"num_hidden_layers\", self.task_config, \"cross_num_hidden_layers\")\n",
    "            self.cross = CrossModel(cross_config)\n",
    "            # <=== End of Cross Encoder\n",
    "            self.similarity_dense = nn.Linear(cross_config.hidden_size, 1)\n",
    "\n",
    "        if self.sim_header == \"seqLSTM\" or self.sim_header == \"seqTransf\":\n",
    "            self.frame_position_embeddings = nn.Embedding(cross_config.max_position_embeddings, cross_config.hidden_size)\n",
    "        if self.sim_header == \"seqTransf\":\n",
    "            self.transformerClip = TransformerClip(width=transformer_width, layers=self.task_config.cross_num_hidden_layers,\n",
    "                                                   heads=transformer_heads, )\n",
    "        if self.sim_header == \"seqLSTM\":\n",
    "            self.lstm_visual = nn.LSTM(input_size=cross_config.hidden_size, hidden_size=cross_config.hidden_size,\n",
    "                                       batch_first=True, bidirectional=False, num_layers=1)\n",
    "\n",
    "        self.loss_fct = CrossEn()\n",
    "\n",
    "        self.apply(self.init_weights)\n",
    "\n",
    "    # video : 7D\n",
    "    def forward(self, input_ids, token_type_ids, attention_mask, video, video_mask=None):\n",
    "        input_ids = input_ids.view(-1, input_ids.shape[-1])\n",
    "        token_type_ids = token_type_ids.view(-1, token_type_ids.shape[-1])\n",
    "        attention_mask = attention_mask.view(-1, attention_mask.shape[-1])\n",
    "        video_mask = video_mask.view(-1, video_mask.shape[-1])\n",
    "\n",
    "        # T x 3 x H x W\n",
    "        video = torch.as_tensor(video).float()\n",
    "        b, pair, bs, ts, channel, h, w = video.shape\n",
    "        # video : 4D\n",
    "        video = video.view(b * pair * bs * ts, channel, h, w)\n",
    "        video_frame = bs * ts\n",
    "\n",
    "        sequence_output, visual_output = self.get_sequence_visual_output(input_ids, token_type_ids, attention_mask,\n",
    "                                                                         video, video_mask, shaped=True, video_frame=video_frame)\n",
    "\n",
    "        if self.training:\n",
    "            loss = 0.\n",
    "            #seq output : [1 x 512] \n",
    "            #vis output(ours) : [1 x 512]\n",
    "            # decoded output(ours) : [1, 16, 3, 224, 224]\n",
    "            #\n",
    "            sim_matrix, *_tmp = self.get_similarity_logits(sequence_output, visual_output, attention_mask, video_mask,\n",
    "                                                    shaped=True, loose_type=self.loose_type)\n",
    "            sim_loss1 = self.loss_fct(sim_matrix)\n",
    "            sim_loss2 = self.loss_fct(sim_matrix.T)\n",
    "            sim_loss = (sim_loss1 + sim_loss2) / 2\n",
    "            loss += sim_loss\n",
    "\n",
    "            return loss\n",
    "        else:\n",
    "            return None\n",
    "\n",
    "    def get_sequence_output(self, input_ids, token_type_ids, attention_mask, shaped=False):\n",
    "        if shaped is False:\n",
    "            input_ids = input_ids.view(-1, input_ids.shape[-1])\n",
    "            token_type_ids = token_type_ids.view(-1, token_type_ids.shape[-1])\n",
    "            attention_mask = attention_mask.view(-1, attention_mask.shape[-1])\n",
    "\n",
    "        bs_pair = input_ids.size(0)\n",
    "        sequence_hidden = self.clip.encode_text(input_ids).float()\n",
    "        sequence_hidden = sequence_hidden.view(bs_pair, -1, sequence_hidden.size(-1))\n",
    "\n",
    "        return sequence_hidden\n",
    "\n",
    "    def get_visual_output(self, video, video_mask, shaped=False, video_frame=-1):\n",
    "        if shaped is False:\n",
    "            video_mask = video_mask.view(-1, video_mask.shape[-1])\n",
    "            video = torch.as_tensor(video).float()\n",
    "            b, pair, bs, ts, channel, h, w = video.shape\n",
    "            video = video.view(b * pair * bs * ts, channel, h, w)\n",
    "            video_frame = bs * ts\n",
    "        # video : 4D\n",
    "        # video = video.view(b * pair * bs * ts, channel, h, w)\n",
    "        bs_pair = video_mask.size(0) # bs_pair : b * pair * bs * ts\n",
    "\n",
    "        ##################### 20220607 ###########################\n",
    "        visual_hidden = self.clip.encode_image(video, video_frame=video_frame).float()\n",
    "\n",
    "        # Case 1 : clip에 정의 되어 있는 encode_imae를 쓰지 않고, 상속 \n",
    "\n",
    "        # Case 2 : clip을 변경 \n",
    "\n",
    "\n",
    "        visual_hidden = visual_hidden.view(bs_pair, -1, visual_hidden.size(-1))\n",
    "\n",
    "        return visual_hidden\n",
    "    #video 4D\n",
    "    def get_sequence_visual_output(self, input_ids, token_type_ids, attention_mask, video, video_mask, shaped=False, video_frame=-1):\n",
    "        if shaped is False:\n",
    "            input_ids = input_ids.view(-1, input_ids.shape[-1])\n",
    "            token_type_ids = token_type_ids.view(-1, token_type_ids.shape[-1])\n",
    "            attention_mask = attention_mask.view(-1, attention_mask.shape[-1])\n",
    "            video_mask = video_mask.view(-1, video_mask.shape[-1])\n",
    "\n",
    "            video = torch.as_tensor(video).float()\n",
    "            b, pair, bs, ts, channel, h, w = video.shape\n",
    "            video = video.view(b * pair * bs * ts, channel, h, w)\n",
    "            video_frame = bs * ts\n",
    "\n",
    "        sequence_output = self.get_sequence_output(input_ids, token_type_ids, attention_mask, shaped=True)\n",
    "        visual_output = self.get_visual_output(video, video_mask, shaped=True, video_frame=video_frame)\n",
    "\n",
    "        return sequence_output, visual_output # v : [16 x 512]\n",
    "    \n",
    "\n",
    "    #두 서로 다른 모달리티를 더하기 \n",
    "    def _get_cross_output(self, sequence_output, visual_output, attention_mask, video_mask):\n",
    "\n",
    "        concat_features = torch.cat((sequence_output, visual_output), dim=1)  # concatnate tokens and frames\n",
    "        concat_mask = torch.cat((attention_mask, video_mask), dim=1)\n",
    "        text_type_ = torch.zeros_like(attention_mask)\n",
    "        video_type_ = torch.ones_like(video_mask)\n",
    "        concat_type = torch.cat((text_type_, video_type_), dim=1)\n",
    "\n",
    "        cross_layers, pooled_output = self.cross(concat_features, concat_type, concat_mask, output_all_encoded_layers=True)\n",
    "        cross_output = cross_layers[-1]\n",
    "\n",
    "        return cross_output, pooled_output, concat_mask\n",
    "\n",
    "    def _mean_pooling_for_similarity_sequence(self, sequence_output, attention_mask):\n",
    "        attention_mask_un = attention_mask.to(dtype=torch.float).unsqueeze(-1)\n",
    "        attention_mask_un[:, 0, :] = 0.\n",
    "        sequence_output = sequence_output * attention_mask_un\n",
    "        text_out = torch.sum(sequence_output, dim=1) / torch.sum(attention_mask_un, dim=1, dtype=torch.float)\n",
    "        return text_out\n",
    "\n",
    "    def _mean_pooling_for_similarity_visual(self, visual_output, video_mask,):\n",
    "        video_mask_un = video_mask.to(dtype=torch.float).unsqueeze(-1)\n",
    "        visual_output = visual_output * video_mask_un\n",
    "        video_mask_un_sum = torch.sum(video_mask_un, dim=1, dtype=torch.float)\n",
    "        video_mask_un_sum[video_mask_un_sum == 0.] = 1.\n",
    "        video_out = torch.sum(visual_output, dim=1) / video_mask_un_sum\n",
    "        return video_out\n",
    "\n",
    "    def _mean_pooling_for_similarity(self, sequence_output, visual_output, attention_mask, video_mask,):\n",
    "        text_out = self._mean_pooling_for_similarity_sequence(sequence_output, attention_mask)\n",
    "        video_out = self._mean_pooling_for_similarity_visual(visual_output, video_mask)\n",
    "\n",
    "        return text_out, video_out\n",
    "\n",
    "    def _loose_similarity(self, sequence_output, visual_output, attention_mask, video_mask, sim_header=\"meanP\"):\n",
    "        sequence_output, visual_output = sequence_output.contiguous(), visual_output.contiguous()\n",
    "\n",
    "        if sim_header == \"meanP\":\n",
    "            # Default: Parameter-free type\n",
    "            pass\n",
    "        elif sim_header == \"seqLSTM\":\n",
    "            # Sequential type: LSTM\n",
    "            visual_output_original = visual_output\n",
    "            visual_output = pack_padded_sequence(visual_output, torch.sum(video_mask, dim=-1).cpu(),\n",
    "                                                 batch_first=True, enforce_sorted=False)\n",
    "            visual_output, _ = self.lstm_visual(visual_output)\n",
    "            if self.training: self.lstm_visual.flatten_parameters()\n",
    "            visual_output, _ = pad_packed_sequence(visual_output, batch_first=True)\n",
    "            visual_output = torch.cat((visual_output, visual_output_original[:, visual_output.size(1):, ...].contiguous()), dim=1)\n",
    "            visual_output = visual_output + visual_output_original\n",
    "        elif sim_header == \"seqTransf\":\n",
    "            # Sequential type: Transformer Encoder\n",
    "            visual_output_original = visual_output\n",
    "            seq_length = visual_output.size(1)\n",
    "            position_ids = torch.arange(seq_length, dtype=torch.long, device=visual_output.device)\n",
    "            position_ids = position_ids.unsqueeze(0).expand(visual_output.size(0), -1)\n",
    "            frame_position_embeddings = self.frame_position_embeddings(position_ids)\n",
    "            visual_output = visual_output + frame_position_embeddings\n",
    "\n",
    "            extended_video_mask = (1.0 - video_mask.unsqueeze(1)) * -1000000.0\n",
    "            extended_video_mask = extended_video_mask.expand(-1, video_mask.size(1), -1)\n",
    "            visual_output = visual_output.permute(1, 0, 2)  # NLD -> LND\n",
    "            visual_output = self.transformerClip(visual_output, extended_video_mask)\n",
    "            visual_output = visual_output.permute(1, 0, 2)  # LND -> NLD\n",
    "            visual_output = visual_output + visual_output_original\n",
    "\n",
    "        if self.training:\n",
    "            visual_output = allgather(visual_output, self.task_config)\n",
    "            video_mask = allgather(video_mask, self.task_config)\n",
    "            sequence_output = allgather(sequence_output, self.task_config)\n",
    "            torch.distributed.barrier()\n",
    "\n",
    "        visual_output = visual_output / visual_output.norm(dim=-1, keepdim=True)\n",
    "        visual_output = self._mean_pooling_for_similarity_visual(visual_output, video_mask)\n",
    "        visual_output = visual_output / visual_output.norm(dim=-1, keepdim=True)\n",
    "\n",
    "        sequence_output = sequence_output.squeeze(1)\n",
    "        sequence_output = sequence_output / sequence_output.norm(dim=-1, keepdim=True)\n",
    "\n",
    "        logit_scale = self.clip.logit_scale.exp()\n",
    "        retrieve_logits = logit_scale * torch.matmul(sequence_output, visual_output.t())\n",
    "        return retrieve_logits\n",
    "\n",
    "    def _cross_similarity(self, sequence_output, visual_output, attention_mask, video_mask):\n",
    "        sequence_output, visual_output = sequence_output.contiguous(), visual_output.contiguous()\n",
    "\n",
    "        b_text, s_text, h_text = sequence_output.size()\n",
    "        b_visual, s_visual, h_visual = visual_output.size()\n",
    "\n",
    "        retrieve_logits_list = []\n",
    "\n",
    "        step_size = b_text      # set smaller to reduce memory cost\n",
    "        split_size = [step_size] * (b_text // step_size)\n",
    "        release_size = b_text - sum(split_size)\n",
    "        if release_size > 0:\n",
    "            split_size += [release_size]\n",
    "\n",
    "        # due to clip text branch retrun the last hidden\n",
    "        attention_mask = torch.ones(sequence_output.size(0), 1)\\\n",
    "            .to(device=attention_mask.device, dtype=attention_mask.dtype)\n",
    "\n",
    "        sequence_output_splits = torch.split(sequence_output, split_size, dim=0)\n",
    "        attention_mask_splits = torch.split(attention_mask, split_size, dim=0)\n",
    "        for i in range(len(split_size)):\n",
    "            sequence_output_row = sequence_output_splits[i]\n",
    "            attention_mask_row = attention_mask_splits[i]\n",
    "            sequence_output_l = sequence_output_row.unsqueeze(1).repeat(1, b_visual, 1, 1)\n",
    "            sequence_output_l = sequence_output_l.view(-1, s_text, h_text)\n",
    "            attention_mask_l = attention_mask_row.unsqueeze(1).repeat(1, b_visual, 1)\n",
    "            attention_mask_l = attention_mask_l.view(-1, s_text)\n",
    "\n",
    "            step_truth = sequence_output_row.size(0)\n",
    "            visual_output_r = visual_output.unsqueeze(0).repeat(step_truth, 1, 1, 1)\n",
    "            visual_output_r = visual_output_r.view(-1, s_visual, h_visual)\n",
    "            video_mask_r = video_mask.unsqueeze(0).repeat(step_truth, 1, 1)\n",
    "            video_mask_r = video_mask_r.view(-1, s_visual)\n",
    "\n",
    "            cross_output, pooled_output, concat_mask = \\\n",
    "                self._get_cross_output(sequence_output_l, visual_output_r, attention_mask_l, video_mask_r)\n",
    "            retrieve_logits_row = self.similarity_dense(pooled_output).squeeze(-1).view(step_truth, b_visual)\n",
    "\n",
    "            retrieve_logits_list.append(retrieve_logits_row)\n",
    "\n",
    "        retrieve_logits = torch.cat(retrieve_logits_list, dim=0)\n",
    "        return retrieve_logits\n",
    "\n",
    "    def get_similarity_logits(self, sequence_output, visual_output, attention_mask, video_mask, shaped=False, loose_type=False):\n",
    "        if shaped is False:\n",
    "            attention_mask = attention_mask.view(-1, attention_mask.shape[-1])\n",
    "            video_mask = video_mask.view(-1, video_mask.shape[-1])\n",
    "\n",
    "        contrastive_direction = ()\n",
    "        if loose_type:\n",
    "            assert self.sim_header in [\"meanP\", \"seqLSTM\", \"seqTransf\"]\n",
    "            retrieve_logits = self._loose_similarity(sequence_output, visual_output, attention_mask, video_mask, sim_header=self.sim_header)\n",
    "        else:\n",
    "            assert self.sim_header in [\"tightTransf\"]\n",
    "            retrieve_logits = self._cross_similarity(sequence_output, visual_output, attention_mask, video_mask, )\n",
    "\n",
    "        return retrieve_logits, contrastive_direction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Stage-One:True, Stage-Two:False\n",
      "\t embed_dim: 512\n",
      "\t image_resolution: 224\n",
      "\t vision_layers: 12\n",
      "\t vision_width: 768\n",
      "\t vision_patch_size: 32\n",
      "\t context_length: 77\n",
      "\t vocab_size: 49408\n",
      "\t transformer_width: 512\n",
      "\t transformer_heads: 8\n",
      "\t transformer_layers: 12\n",
      "\t cut_top_layer: 0\n",
      "Stage-One:True, Stage-Two:False\n",
      "\t embed_dim: 512\n",
      "\t image_resolution: 224\n",
      "\t vision_layers: 12\n",
      "\t vision_width: 768\n",
      "\t vision_patch_size: 32\n",
      "\t context_length: 77\n",
      "\t vocab_size: 49408\n",
      "\t transformer_width: 512\n",
      "\t transformer_heads: 8\n",
      "\t transformer_layers: 12\n",
      "\t cut_top_layer: 0\n"
     ]
    }
   ],
   "source": [
    "c4c = CLIP4Clip(cross_config,clip_state_dict,task_config)\n",
    "vaetc4c = VAeTCLIP4Clip(cross_config,clip_state_dict,task_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([768, 3, 32, 32])"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clip_state_dict['visual.conv1.weight'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@property\n",
    "def dtype(self):\n",
    "    return self.visual.conv1.weight.dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.float32\n"
     ]
    }
   ],
   "source": [
    "print(clip.visual.conv1.weight.dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.float32\n"
     ]
    }
   ],
   "source": [
    "print(vaetclip.visual.conv1.weight.dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch.utils.data.dataloader.DataLoader at 0x7f564ecf4fa0>"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.13 ('CLIP4Clip': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "eff65c0def8c12c365a8ff92cc515e5fbadd507d935c66604f35c789f0fc4099"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
